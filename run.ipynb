{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13269,
     "status": "ok",
     "timestamp": 1749612119388,
     "user": {
      "displayName": "Hank Fang",
      "userId": "09766178429983897841"
     },
     "user_tz": 420
    },
    "id": "0UtOSeInB2Cr",
    "outputId": "d10731f2-efd6-4d69-fe8f-ef4b72118f7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Added '/content/drive/MyDrive/cse493s' to sys.path\n",
      "Changed current working directory to: /content/drive/MyDrive/cse493s\n",
      "\n",
      "Files in the current working directory (should be your project folder):\n",
      "data\t\t  inference.py\tout\t     train.py\n",
      "generate_data.py  model.py\t__pycache__  Untitled0.ipynb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "FOLDERNAME_IN_MY_DRIVE = 'cse493s'  # <--- CHANGE THIS TO YOUR ACTUAL FOLDER PATH\n",
    "assert FOLDERNAME_IN_MY_DRIVE is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Construct the full path to your project folder\n",
    "PROJECT_FOLDER_PATH = os.path.join('/content/drive/MyDrive/', FOLDERNAME_IN_MY_DRIVE)\n",
    "\n",
    "if PROJECT_FOLDER_PATH not in sys.path:\n",
    "    sys.path.append(PROJECT_FOLDER_PATH)\n",
    "    print(f\"Added '{PROJECT_FOLDER_PATH}' to sys.path\")\n",
    "else:\n",
    "    print(f\"'{PROJECT_FOLDER_PATH}' is already in sys.path\")\n",
    "\n",
    "try:\n",
    "    os.chdir(PROJECT_FOLDER_PATH)\n",
    "    print(f\"Changed current working directory to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] The folder '{PROJECT_FOLDER_PATH}' was not found. Please check your FOLDERNAME_IN_MY_DRIVE.\")\n",
    "    # You might want to stop execution here or handle the error appropriately\n",
    "\n",
    "# Verify by listing files in the current directory\n",
    "print(\"\\nFiles in the current working directory (should be your project folder):\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ipyparallel module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipyparallel\n",
    "from ipyparallel import Client\n",
    "rc = Client()\n",
    "dview = rc[:]\n",
    "dview.block = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1513,
     "status": "ok",
     "timestamp": 1749579971249,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "1Z9_BXY1JYOQ",
    "outputId": "e9d6db9f-2c33-4948-9b71-e6b386de00e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 equations to data/sanity_check/train.txt\n",
      "Saved 20 equations to data/sanity_check/val.txt\n",
      "Saved 20 equations to data/sanity_check/test.txt\n",
      "\n",
      "Generated sanity check data in data/sanity_check\n"
     ]
    }
   ],
   "source": [
    "!python generate_data.py --sanity_check --output_dir data/sanity_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20474,
     "status": "ok",
     "timestamp": 1749579992421,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "3HBf-rOOJfMU",
    "outputId": "2c2c8288-41a5-435a-db30-d741d44787f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/sanity_check\n",
      "Vocabulary size: 15\n",
      "Initializing 1-layer model\n",
      "number of parameters: 0.01M\n",
      "num decayed parameter tensors: 6, with 13,792 parameters\n",
      "num non-decayed parameter tensors: 3, with 96 parameters\n",
      "using fused AdamW: True\n",
      "/content/drive/MyDrive/cse493s/train.py:200: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "Starting training for 1000 steps\n",
      "Step 0: train loss 2.7670, train acc 0.0000\n",
      "Step 0: val loss 2.7379, val acc 0.0000\n",
      "Step 10: train loss 2.5085, train acc 0.1364\n",
      "Step 20: train loss 2.3168, train acc 0.5000\n",
      "Step 30: train loss 2.1729, train acc 0.9545\n",
      "Step 40: train loss 2.0515, train acc 1.0000\n",
      "Step 50: train loss 1.9524, train acc 1.0000\n",
      "Step 60: train loss 1.8636, train acc 1.0000\n",
      "Step 70: train loss 1.7805, train acc 1.0000\n",
      "Step 80: train loss 1.6967, train acc 1.0000\n",
      "Step 90: train loss 1.6186, train acc 1.0000\n",
      "Step 100: train loss 1.5380, train acc 1.0000\n",
      "Step 100: val loss 1.5305, val acc 1.0000\n",
      "Step 110: train loss 1.4602, train acc 1.0000\n",
      "Step 120: train loss 1.3835, train acc 1.0000\n",
      "Step 130: train loss 1.3082, train acc 1.0000\n",
      "Step 140: train loss 1.2337, train acc 1.0000\n",
      "Step 150: train loss 1.1598, train acc 1.0000\n",
      "Step 160: train loss 1.0881, train acc 1.0000\n",
      "Step 170: train loss 1.0210, train acc 1.0000\n",
      "Step 180: train loss 0.9537, train acc 1.0000\n",
      "Step 190: train loss 0.8894, train acc 1.0000\n",
      "Step 200: train loss 0.8278, train acc 1.0000\n",
      "Step 200: val loss 0.8219, val acc 1.0000\n",
      "Step 210: train loss 0.7688, train acc 1.0000\n",
      "Step 220: train loss 0.7120, train acc 1.0000\n",
      "Step 230: train loss 0.6591, train acc 1.0000\n",
      "Step 240: train loss 0.6100, train acc 1.0000\n",
      "Step 250: train loss 0.5628, train acc 1.0000\n",
      "Step 260: train loss 0.5187, train acc 1.0000\n",
      "Step 270: train loss 0.4782, train acc 1.0000\n",
      "Step 280: train loss 0.4402, train acc 1.0000\n",
      "Step 290: train loss 0.4046, train acc 1.0000\n",
      "Step 300: train loss 0.3709, train acc 1.0000\n",
      "Step 300: val loss 0.3685, val acc 1.0000\n",
      "Step 310: train loss 0.3402, train acc 1.0000\n",
      "Step 320: train loss 0.3117, train acc 1.0000\n",
      "Step 330: train loss 0.2852, train acc 1.0000\n",
      "Step 340: train loss 0.2603, train acc 1.0000\n",
      "Step 350: train loss 0.2376, train acc 1.0000\n",
      "Step 360: train loss 0.2166, train acc 1.0000\n",
      "Step 370: train loss 0.1971, train acc 1.0000\n",
      "Step 380: train loss 0.1792, train acc 1.0000\n",
      "Step 390: train loss 0.1631, train acc 1.0000\n",
      "Step 400: train loss 0.1487, train acc 1.0000\n",
      "Step 400: val loss 0.1460, val acc 1.0000\n",
      "Step 410: train loss 0.1344, train acc 1.0000\n",
      "Step 420: train loss 0.1216, train acc 1.0000\n",
      "Step 430: train loss 0.1104, train acc 1.0000\n",
      "Step 440: train loss 0.1002, train acc 1.0000\n",
      "Step 450: train loss 0.0905, train acc 1.0000\n",
      "Step 460: train loss 0.0818, train acc 1.0000\n",
      "Step 470: train loss 0.0739, train acc 1.0000\n",
      "Step 480: train loss 0.0671, train acc 1.0000\n",
      "Step 490: train loss 0.0605, train acc 1.0000\n",
      "Step 500: train loss 0.0545, train acc 1.0000\n",
      "Step 500: val loss 0.0541, val acc 1.0000\n",
      "Step 510: train loss 0.0493, train acc 1.0000\n",
      "Step 520: train loss 0.0443, train acc 1.0000\n",
      "Step 530: train loss 0.0401, train acc 1.0000\n",
      "Step 540: train loss 0.0361, train acc 1.0000\n",
      "Step 550: train loss 0.0324, train acc 1.0000\n",
      "Step 560: train loss 0.0293, train acc 1.0000\n",
      "Step 570: train loss 0.0263, train acc 1.0000\n",
      "Step 580: train loss 0.0237, train acc 1.0000\n",
      "Step 590: train loss 0.0213, train acc 1.0000\n",
      "Step 600: train loss 0.0191, train acc 1.0000\n",
      "Step 600: val loss 0.0189, val acc 1.0000\n",
      "Step 610: train loss 0.0172, train acc 1.0000\n",
      "Step 620: train loss 0.0154, train acc 1.0000\n",
      "Step 630: train loss 0.0139, train acc 1.0000\n",
      "Step 640: train loss 0.0124, train acc 1.0000\n",
      "Step 650: train loss 0.0112, train acc 1.0000\n",
      "Step 660: train loss 0.0100, train acc 1.0000\n",
      "Step 670: train loss 0.0090, train acc 1.0000\n",
      "Step 680: train loss 0.0080, train acc 1.0000\n",
      "Step 690: train loss 0.0072, train acc 1.0000\n",
      "Step 700: train loss 0.0065, train acc 1.0000\n",
      "Step 700: val loss 0.0064, val acc 1.0000\n",
      "Step 710: train loss 0.0058, train acc 1.0000\n",
      "Step 720: train loss 0.0052, train acc 1.0000\n",
      "Step 730: train loss 0.0046, train acc 1.0000\n",
      "Step 740: train loss 0.0041, train acc 1.0000\n",
      "Step 750: train loss 0.0037, train acc 1.0000\n",
      "Step 760: train loss 0.0033, train acc 1.0000\n",
      "Step 770: train loss 0.0030, train acc 1.0000\n",
      "Step 780: train loss 0.0026, train acc 1.0000\n",
      "Step 790: train loss 0.0024, train acc 1.0000\n",
      "Step 800: train loss 0.0021, train acc 1.0000\n",
      "Step 800: val loss 0.0021, val acc 1.0000\n",
      "Step 810: train loss 0.0019, train acc 1.0000\n",
      "Step 820: train loss 0.0017, train acc 1.0000\n",
      "Step 830: train loss 0.0015, train acc 1.0000\n",
      "Step 840: train loss 0.0013, train acc 1.0000\n",
      "Step 850: train loss 0.0012, train acc 1.0000\n",
      "Step 860: train loss 0.0011, train acc 1.0000\n",
      "Step 870: train loss 0.0009, train acc 1.0000\n",
      "Step 880: train loss 0.0008, train acc 1.0000\n",
      "Step 890: train loss 0.0007, train acc 1.0000\n",
      "Step 900: train loss 0.0007, train acc 1.0000\n",
      "Step 900: val loss 0.0007, val acc 1.0000\n",
      "Step 910: train loss 0.0006, train acc 1.0000\n",
      "Step 920: train loss 0.0005, train acc 1.0000\n",
      "Step 930: train loss 0.0005, train acc 1.0000\n",
      "Step 940: train loss 0.0004, train acc 1.0000\n",
      "Step 950: train loss 0.0004, train acc 1.0000\n",
      "Step 960: train loss 0.0003, train acc 1.0000\n",
      "Step 970: train loss 0.0003, train acc 1.0000\n",
      "Step 980: train loss 0.0003, train acc 1.0000\n",
      "Step 990: train loss 0.0002, train acc 1.0000\n",
      "Training completed!\n",
      "Saved final model to out/sanity_check/final_model.pt\n",
      "Saved training curves to out/sanity_check/training_curves.png\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "    --data_dir data/sanity_check \\\n",
    "    --out_dir out/sanity_check \\\n",
    "    --n_layer 1 \\\n",
    "    --n_embd 32 \\\n",
    "    --n_head 4 \\\n",
    "    --max_steps 1000 \\\n",
    "    --log_interval 10 \\\n",
    "    --eval_interval 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14359,
     "status": "ok",
     "timestamp": 1749580023048,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "8VzodUo0LDvV",
    "outputId": "79c7f701-35cc-454f-b32a-bd9ead17d5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/sanity_check\n",
      "Vocabulary size: 15\n",
      "Initializing 1-layer model\n",
      "number of parameters: 0.01M\n",
      "num decayed parameter tensors: 6, with 13,792 parameters\n",
      "num non-decayed parameter tensors: 3, with 96 parameters\n",
      "using fused AdamW: True\n",
      "/content/drive/MyDrive/cse493s/train.py:200: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "Starting training for 1000 steps\n",
      "Step 0: train loss 2.7633, train acc 0.0000\n",
      "Step 0: val loss 2.7344, val acc 0.0000\n",
      "Step 10: train loss 2.4984, train acc 0.1000\n",
      "Step 20: train loss 2.3031, train acc 0.5500\n",
      "Step 30: train loss 2.1609, train acc 0.9500\n",
      "Step 40: train loss 2.0457, train acc 1.0000\n",
      "Step 50: train loss 1.9488, train acc 1.0000\n",
      "Step 60: train loss 1.8613, train acc 1.0000\n",
      "Step 70: train loss 1.7789, train acc 1.0000\n",
      "Step 80: train loss 1.6980, train acc 1.0000\n",
      "Step 90: train loss 1.6180, train acc 1.0000\n",
      "Step 100: train loss 1.5383, train acc 1.0000\n",
      "Step 100: val loss 1.5309, val acc 1.0000\n",
      "Step 110: train loss 1.4609, train acc 1.0000\n",
      "Step 120: train loss 1.3828, train acc 1.0000\n",
      "Step 130: train loss 1.3090, train acc 1.0000\n",
      "Step 140: train loss 1.2336, train acc 1.0000\n",
      "Step 150: train loss 1.1609, train acc 1.0000\n",
      "Step 160: train loss 1.0922, train acc 1.0000\n",
      "Step 170: train loss 1.0217, train acc 1.0000\n",
      "Step 180: train loss 0.9549, train acc 1.0000\n",
      "Step 190: train loss 0.8920, train acc 1.0000\n",
      "Step 200: train loss 0.8307, train acc 1.0000\n",
      "Step 200: val loss 0.8246, val acc 1.0000\n",
      "Step 210: train loss 0.7709, train acc 1.0000\n",
      "Step 220: train loss 0.7146, train acc 1.0000\n",
      "Step 230: train loss 0.6611, train acc 1.0000\n",
      "Step 240: train loss 0.6123, train acc 1.0000\n",
      "Step 250: train loss 0.5645, train acc 1.0000\n",
      "Step 260: train loss 0.5211, train acc 1.0000\n",
      "Step 270: train loss 0.4808, train acc 1.0000\n",
      "Step 280: train loss 0.4420, train acc 1.0000\n",
      "Step 290: train loss 0.4060, train acc 1.0000\n",
      "Step 300: train loss 0.3729, train acc 1.0000\n",
      "Step 300: val loss 0.3690, val acc 1.0000\n",
      "Step 310: train loss 0.3429, train acc 1.0000\n",
      "Step 320: train loss 0.3127, train acc 1.0000\n",
      "Step 330: train loss 0.2863, train acc 1.0000\n",
      "Step 340: train loss 0.2611, train acc 1.0000\n",
      "Step 350: train loss 0.2387, train acc 1.0000\n",
      "Step 360: train loss 0.2175, train acc 1.0000\n",
      "Step 370: train loss 0.1984, train acc 1.0000\n",
      "Step 380: train loss 0.1807, train acc 1.0000\n",
      "Step 390: train loss 0.1637, train acc 1.0000\n",
      "Step 400: train loss 0.1490, train acc 1.0000\n",
      "Step 400: val loss 0.1480, val acc 1.0000\n",
      "Step 410: train loss 0.1355, train acc 1.0000\n",
      "Step 420: train loss 0.1233, train acc 1.0000\n",
      "Step 430: train loss 0.1117, train acc 1.0000\n",
      "Step 440: train loss 0.1008, train acc 1.0000\n",
      "Step 450: train loss 0.0913, train acc 1.0000\n",
      "Step 460: train loss 0.0827, train acc 1.0000\n",
      "Step 470: train loss 0.0749, train acc 1.0000\n",
      "Step 480: train loss 0.0674, train acc 1.0000\n",
      "Step 490: train loss 0.0611, train acc 1.0000\n",
      "Step 500: train loss 0.0552, train acc 1.0000\n",
      "Step 500: val loss 0.0547, val acc 1.0000\n",
      "Step 510: train loss 0.0499, train acc 1.0000\n",
      "Step 520: train loss 0.0449, train acc 1.0000\n",
      "Step 530: train loss 0.0406, train acc 1.0000\n",
      "Step 540: train loss 0.0364, train acc 1.0000\n",
      "Step 550: train loss 0.0328, train acc 1.0000\n",
      "Step 560: train loss 0.0297, train acc 1.0000\n",
      "Step 570: train loss 0.0267, train acc 1.0000\n",
      "Step 580: train loss 0.0239, train acc 1.0000\n",
      "Step 590: train loss 0.0217, train acc 1.0000\n",
      "Step 600: train loss 0.0193, train acc 1.0000\n",
      "Step 600: val loss 0.0192, val acc 1.0000\n",
      "Step 610: train loss 0.0174, train acc 1.0000\n",
      "Step 620: train loss 0.0157, train acc 1.0000\n",
      "Step 630: train loss 0.0140, train acc 1.0000\n",
      "Step 640: train loss 0.0126, train acc 1.0000\n",
      "Step 650: train loss 0.0113, train acc 1.0000\n",
      "Step 660: train loss 0.0102, train acc 1.0000\n",
      "Step 670: train loss 0.0091, train acc 1.0000\n",
      "Step 680: train loss 0.0081, train acc 1.0000\n",
      "Step 690: train loss 0.0073, train acc 1.0000\n",
      "Step 700: train loss 0.0065, train acc 1.0000\n",
      "Step 700: val loss 0.0064, val acc 1.0000\n",
      "Step 710: train loss 0.0058, train acc 1.0000\n",
      "Step 720: train loss 0.0052, train acc 1.0000\n",
      "Step 730: train loss 0.0047, train acc 1.0000\n",
      "Step 740: train loss 0.0042, train acc 1.0000\n",
      "Step 750: train loss 0.0038, train acc 1.0000\n",
      "Step 760: train loss 0.0033, train acc 1.0000\n",
      "Step 770: train loss 0.0030, train acc 1.0000\n",
      "Step 780: train loss 0.0027, train acc 1.0000\n",
      "Step 790: train loss 0.0024, train acc 1.0000\n",
      "Step 800: train loss 0.0021, train acc 1.0000\n",
      "Step 800: val loss 0.0021, val acc 1.0000\n",
      "Step 810: train loss 0.0019, train acc 1.0000\n",
      "Step 820: train loss 0.0017, train acc 1.0000\n",
      "Step 830: train loss 0.0015, train acc 1.0000\n",
      "Step 840: train loss 0.0013, train acc 1.0000\n",
      "Step 850: train loss 0.0012, train acc 1.0000\n",
      "Step 860: train loss 0.0011, train acc 1.0000\n",
      "Step 870: train loss 0.0009, train acc 1.0000\n",
      "Step 880: train loss 0.0008, train acc 1.0000\n",
      "Step 890: train loss 0.0007, train acc 1.0000\n",
      "Step 900: train loss 0.0007, train acc 1.0000\n",
      "Step 900: val loss 0.0007, val acc 1.0000\n",
      "Step 910: train loss 0.0006, train acc 1.0000\n",
      "Step 920: train loss 0.0005, train acc 1.0000\n",
      "Step 930: train loss 0.0005, train acc 1.0000\n",
      "Step 940: train loss 0.0004, train acc 1.0000\n",
      "Step 950: train loss 0.0004, train acc 1.0000\n",
      "Step 960: train loss 0.0003, train acc 1.0000\n",
      "Step 970: train loss 0.0003, train acc 1.0000\n",
      "Step 980: train loss 0.0003, train acc 1.0000\n",
      "Step 990: train loss 0.0002, train acc 1.0000\n",
      "Training completed!\n",
      "Saved final model to out/sanity_check_masked/final_model.pt\n",
      "Saved training curves to out/sanity_check_masked/training_curves.png\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "    --data_dir data/sanity_check \\\n",
    "    --out_dir out/sanity_check_masked \\\n",
    "    --n_layer 1 \\\n",
    "    --n_embd 32 \\\n",
    "    --n_head 4 \\\n",
    "    --max_steps 1000 \\\n",
    "    --log_interval 10 \\\n",
    "    --eval_interval 100 \\\n",
    "    --mask_first_n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3522,
     "status": "ok",
     "timestamp": 1749580047638,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "D36nLAUqLS1_",
    "outputId": "0ab54234-beb7-4b5b-ce14-8878226fce95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from out/sanity_check/final_model.pt\n",
      "Loaded tokenizer with vocabulary size: 15\n",
      "number of parameters: 0.01M\n",
      "Loaded model with 1 layers, 32 dimensions\n",
      "\n",
      "Prompt 1: 'I'\n",
      "Generated: I love machine learning\n",
      "--------------------------------------------------\n",
      "\n",
      "Inference completed!\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "    --checkpoint out/sanity_check/final_model.pt \\\n",
    "    --prompts \"I\" \\\n",
    "    --max_new_tokens 22 \\\n",
    "    --temperature 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3016,
     "status": "ok",
     "timestamp": 1749580051388,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "iGGa5CGgMnWd",
    "outputId": "b091e340-dc9a-4018-c838-88ecafce3269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from out/sanity_check_masked/final_model.pt\n",
      "Loaded tokenizer with vocabulary size: 15\n",
      "number of parameters: 0.01M\n",
      "Loaded model with 1 layers, 32 dimensions\n",
      "\n",
      "Prompt 1: 'I l'\n",
      "Generated: I love machine learning\n",
      "--------------------------------------------------\n",
      "\n",
      "Inference completed!\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "    --checkpoint out/sanity_check_masked/final_model.pt \\\n",
    "    --prompts \"I l\" \\\n",
    "    --max_new_tokens 20 \\\n",
    "    --temperature 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1749145568173,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "YlB0i-D1MyUg",
    "outputId": "b23fa0ce-78ea-4fea-ec36-abf8fbe5b809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating add data with modulus 97\n",
      "Saved 6586 equations to data/algorithmic/add_mod97/train.txt\n",
      "Saved 1411 equations to data/algorithmic/add_mod97/val.txt\n",
      "Saved 1412 equations to data/algorithmic/add_mod97/test.txt\n",
      "Total equations: 9409\n",
      "Train: 6586, Val: 1411, Test: 1412\n",
      "Example equations:\n",
      "  73+93=69\n",
      "  15+74=89\n",
      "  75+47=25\n",
      "  89+93=85\n",
      "  76+27=6\n",
      "\n",
      "Generating add data with modulus 113\n",
      "Saved 8938 equations to data/algorithmic/add_mod113/train.txt\n",
      "Saved 1915 equations to data/algorithmic/add_mod113/val.txt\n",
      "Saved 1916 equations to data/algorithmic/add_mod113/test.txt\n",
      "Total equations: 12769\n",
      "Train: 8938, Val: 1915, Test: 1916\n",
      "Example equations:\n",
      "  71+4=75\n",
      "  99+97=83\n",
      "  7+59=66\n",
      "  102+16=5\n",
      "  1+42=43\n",
      "\n",
      "Generating subtract data with modulus 97\n",
      "Saved 6586 equations to data/algorithmic/subtract_mod97/train.txt\n",
      "Saved 1411 equations to data/algorithmic/subtract_mod97/val.txt\n",
      "Saved 1412 equations to data/algorithmic/subtract_mod97/test.txt\n",
      "Total equations: 9409\n",
      "Train: 6586, Val: 1411, Test: 1412\n",
      "Example equations:\n",
      "  73-93=77\n",
      "  15-74=38\n",
      "  75-47=28\n",
      "  89-93=93\n",
      "  76-27=49\n",
      "\n",
      "Generating subtract data with modulus 113\n",
      "Saved 8938 equations to data/algorithmic/subtract_mod113/train.txt\n",
      "Saved 1915 equations to data/algorithmic/subtract_mod113/val.txt\n",
      "Saved 1916 equations to data/algorithmic/subtract_mod113/test.txt\n",
      "Total equations: 12769\n",
      "Train: 8938, Val: 1915, Test: 1916\n",
      "Example equations:\n",
      "  71-4=67\n",
      "  99-97=2\n",
      "  7-59=61\n",
      "  102-16=86\n",
      "  1-42=72\n",
      "\n",
      "Generating divide data with modulus 97\n",
      "Saved 6518 equations to data/algorithmic/divide_mod97/train.txt\n",
      "Saved 1396 equations to data/algorithmic/divide_mod97/val.txt\n",
      "Saved 1398 equations to data/algorithmic/divide_mod97/test.txt\n",
      "Total equations: 9312\n",
      "Train: 6518, Val: 1396, Test: 1398\n",
      "Example equations:\n",
      "  62/60=56\n",
      "  65/54=3\n",
      "  46/52=27\n",
      "  16/56=28\n",
      "  0/41=0\n",
      "\n",
      "Generating divide data with modulus 113\n",
      "Saved 8859 equations to data/algorithmic/divide_mod113/train.txt\n",
      "Saved 1898 equations to data/algorithmic/divide_mod113/val.txt\n",
      "Saved 1899 equations to data/algorithmic/divide_mod113/test.txt\n",
      "Total equations: 12656\n",
      "Train: 8859, Val: 1898, Test: 1899\n",
      "Example equations:\n",
      "  0/64=0\n",
      "  64/25=102\n",
      "  79/89=105\n",
      "  2/79=93\n",
      "  48/42=98\n"
     ]
    }
   ],
   "source": [
    "!python generate_data.py \\\n",
    "    --operations add,subtract,divide \\\n",
    "    --moduli 97,113 \\\n",
    "    --output_dir data/algorithmic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "px0QnvTdRl6s",
    "outputId": "65b4c8cc-433e-43d9-f6f0-14bed0693a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with seed 42\n",
      "Loading data from data/algorithmic/add_mod97\n",
      "Vocabulary size: 102\n",
      "Initializing 1-layer model\n",
      "number of parameters: 0.21M\n",
      "num decayed parameter tensors: 6, with 213,760 parameters\n",
      "num non-decayed parameter tensors: 3, with 384 parameters\n",
      "using fused AdamW: True\n",
      "/home/xsling/CSE/599s/train.py:242: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "Starting training for 100000 steps\n",
      "Step 0: train loss 4.7021, train acc 0.0000\n",
      "Step 0: val loss 4.6801, val acc 0.0054\n",
      "Step 100: train loss 4.6313, train acc 0.0156\n",
      "Step 200: train loss 4.5825, train acc 0.0000\n",
      "Step 300: train loss 4.5933, train acc 0.0000\n",
      "Step 400: train loss 4.5830, train acc 0.0000\n",
      "Step 500: train loss 4.6040, train acc 0.0156\n",
      "Step 600: train loss 4.5840, train acc 0.0000\n",
      "Step 700: train loss 4.5806, train acc 0.0156\n",
      "Step 800: train loss 4.5762, train acc 0.0312\n",
      "Step 900: train loss 4.5781, train acc 0.0156\n",
      "Step 1000: train loss 4.5830, train acc 0.0000\n",
      "Step 1000: val loss 4.5859, val acc 0.0061\n",
      "Step 1100: train loss 4.5493, train acc 0.0312\n",
      "Step 1200: train loss 4.5913, train acc 0.0156\n",
      "Step 1300: train loss 4.5913, train acc 0.0000\n",
      "Step 1400: train loss 4.5908, train acc 0.0156\n",
      "Step 1500: train loss 4.6001, train acc 0.0312\n",
      "Step 1600: train loss 4.5815, train acc 0.0156\n",
      "Step 1700: train loss 4.5518, train acc 0.0469\n",
      "Step 1800: train loss 4.5786, train acc 0.0000\n",
      "Step 1900: train loss 4.5889, train acc 0.0000\n",
      "Step 2000: train loss 4.5742, train acc 0.0312\n",
      "Step 2000: val loss 4.6334, val acc 0.0020\n",
      "Step 2100: train loss 4.5586, train acc 0.0000\n",
      "Step 2200: train loss 4.5557, train acc 0.0156\n",
      "Step 2300: train loss 4.3889, train acc 0.0156\n",
      "Step 2400: train loss 3.2781, train acc 0.1094\n",
      "Step 2500: train loss 3.0725, train acc 0.0625\n",
      "Step 2600: train loss 2.8430, train acc 0.0938\n",
      "Step 2700: train loss 2.5332, train acc 0.1875\n",
      "Step 2800: train loss 2.3364, train acc 0.2500\n",
      "Step 2900: train loss 2.2889, train acc 0.1875\n",
      "Step 3000: train loss 2.1066, train acc 0.3125\n",
      "Step 3000: val loss 2.1823, val acc 0.1997\n",
      "Step 3100: train loss 1.9861, train acc 0.3281\n",
      "Step 3200: train loss 1.8022, train acc 0.3594\n",
      "Step 3300: train loss 1.8496, train acc 0.3906\n",
      "Step 3400: train loss 1.7897, train acc 0.3125\n",
      "Step 3500: train loss 1.7084, train acc 0.3594\n",
      "Step 3600: train loss 1.6384, train acc 0.3750\n",
      "Step 3700: train loss 1.6926, train acc 0.2500\n",
      "Step 3800: train loss 1.6479, train acc 0.2812\n",
      "Step 3900: train loss 1.5737, train acc 0.3594\n",
      "Step 4000: train loss 1.4719, train acc 0.4844\n",
      "Step 4000: val loss 1.6114, val acc 0.3064\n",
      "Step 4100: train loss 1.4959, train acc 0.3594\n",
      "Step 4200: train loss 1.4457, train acc 0.3281\n",
      "Step 4300: train loss 1.3655, train acc 0.4375\n",
      "Step 4400: train loss 1.4410, train acc 0.4062\n",
      "Step 4500: train loss 1.4052, train acc 0.3594\n",
      "Step 4600: train loss 1.4323, train acc 0.3125\n",
      "Step 4700: train loss 1.2257, train acc 0.5312\n",
      "Step 4800: train loss 1.2969, train acc 0.3906\n",
      "Step 4900: train loss 1.2485, train acc 0.5000\n",
      "Step 5000: train loss 1.3080, train acc 0.4062\n",
      "Step 5000: val loss 1.3143, val acc 0.4049\n",
      "Step 5100: train loss 1.3152, train acc 0.4219\n",
      "Step 5200: train loss 1.1936, train acc 0.5156\n",
      "Step 5300: train loss 1.0947, train acc 0.5781\n",
      "Step 5400: train loss 1.2428, train acc 0.4844\n",
      "Step 5500: train loss 1.0835, train acc 0.5781\n",
      "Step 5600: train loss 1.0562, train acc 0.5469\n",
      "Step 5700: train loss 1.0737, train acc 0.6250\n",
      "Step 5800: train loss 1.1541, train acc 0.4531\n",
      "Step 5900: train loss 1.1929, train acc 0.4062\n",
      "Step 6000: train loss 1.0235, train acc 0.5625\n",
      "Step 6000: val loss 1.2088, val acc 0.4549\n",
      "Step 6100: train loss 1.0442, train acc 0.5625\n",
      "Step 6200: train loss 1.0150, train acc 0.5312\n",
      "Step 6300: train loss 1.0375, train acc 0.5312\n",
      "Step 6400: train loss 1.0748, train acc 0.4531\n",
      "Step 6500: train loss 1.0599, train acc 0.5000\n",
      "Step 6600: train loss 1.0927, train acc 0.5312\n",
      "Step 6700: train loss 0.8911, train acc 0.6562\n",
      "Step 6800: train loss 1.0031, train acc 0.6094\n",
      "Step 6900: train loss 0.9992, train acc 0.5345\n",
      "Step 7000: train loss 1.0538, train acc 0.5000\n",
      "Step 7000: val loss 1.0078, val acc 0.5521\n",
      "Step 7100: train loss 1.0390, train acc 0.5156\n",
      "Step 7200: train loss 0.9826, train acc 0.5625\n",
      "Step 7300: train loss 0.9226, train acc 0.6406\n",
      "Step 7400: train loss 0.9487, train acc 0.6406\n",
      "Step 7500: train loss 0.9383, train acc 0.5312\n",
      "Step 7600: train loss 0.9202, train acc 0.5938\n",
      "Step 7700: train loss 0.8497, train acc 0.6562\n",
      "Step 7800: train loss 0.8787, train acc 0.6719\n",
      "Step 7900: train loss 0.9447, train acc 0.5000\n",
      "Step 8000: train loss 0.9248, train acc 0.5156\n",
      "Step 8000: val loss 0.8854, val acc 0.6268\n",
      "Step 8100: train loss 0.9669, train acc 0.5938\n",
      "Step 8200: train loss 0.8392, train acc 0.5938\n",
      "Step 8300: train loss 0.8954, train acc 0.5938\n",
      "Step 8400: train loss 0.9177, train acc 0.6094\n",
      "Step 8500: train loss 0.9510, train acc 0.5625\n",
      "Step 8600: train loss 0.8697, train acc 0.6094\n",
      "Step 8700: train loss 0.8892, train acc 0.5938\n",
      "Step 8800: train loss 0.8874, train acc 0.6562\n",
      "Step 8900: train loss 0.7921, train acc 0.6406\n",
      "Step 9000: train loss 0.8241, train acc 0.6562\n",
      "Step 9000: val loss 0.8284, val acc 0.6621\n",
      "Step 9100: train loss 0.7316, train acc 0.7812\n",
      "Step 9200: train loss 0.7654, train acc 0.6094\n",
      "Step 9300: train loss 0.7173, train acc 0.7656\n",
      "Step 9400: train loss 0.6967, train acc 0.7500\n",
      "Step 9500: train loss 0.8368, train acc 0.5781\n",
      "Step 9600: train loss 0.6331, train acc 0.7969\n",
      "Step 9700: train loss 0.6498, train acc 0.7812\n",
      "Step 9800: train loss 0.7616, train acc 0.7031\n",
      "Step 9900: train loss 0.6583, train acc 0.6875\n",
      "Step 10000: train loss 0.6514, train acc 0.7031\n",
      "Step 10000: val loss 0.7660, val acc 0.6784\n",
      "Step 10100: train loss 0.7277, train acc 0.7031\n",
      "Step 10200: train loss 0.6789, train acc 0.7188\n",
      "Step 10300: train loss 0.8270, train acc 0.6250\n",
      "Step 10400: train loss 0.9027, train acc 0.5781\n",
      "Step 10500: train loss 0.8486, train acc 0.6562\n",
      "Step 10600: train loss 0.7025, train acc 0.6719\n",
      "Step 10700: train loss 0.7136, train acc 0.7812\n",
      "Step 10800: train loss 0.6239, train acc 0.7500\n",
      "Step 10900: train loss 0.7355, train acc 0.7500\n",
      "Step 11000: train loss 0.6022, train acc 0.7344\n",
      "Step 11000: val loss 0.6822, val acc 0.7253\n",
      "Step 11100: train loss 0.6948, train acc 0.7344\n",
      "Step 11200: train loss 0.5233, train acc 0.8125\n",
      "Step 11300: train loss 0.6525, train acc 0.7188\n",
      "Step 11400: train loss 0.6777, train acc 0.7344\n",
      "Step 11500: train loss 0.6227, train acc 0.7344\n",
      "Step 11600: train loss 0.5917, train acc 0.7656\n",
      "Step 11700: train loss 0.5386, train acc 0.7812\n",
      "Step 11800: train loss 0.6019, train acc 0.7500\n",
      "Step 11900: train loss 0.4985, train acc 0.8594\n",
      "Step 12000: train loss 0.5025, train acc 0.8594\n",
      "Step 12000: val loss 0.6233, val acc 0.7507\n",
      "Step 12100: train loss 0.4731, train acc 0.8281\n",
      "Step 12200: train loss 0.7302, train acc 0.7031\n",
      "Step 12300: train loss 0.7373, train acc 0.7344\n",
      "Step 12400: train loss 0.4840, train acc 0.8125\n",
      "Step 12500: train loss 0.5138, train acc 0.8125\n",
      "Step 12600: train loss 0.5203, train acc 0.8125\n",
      "Step 12700: train loss 0.4432, train acc 0.8906\n",
      "Step 12800: train loss 0.5657, train acc 0.7500\n",
      "Step 12900: train loss 0.5468, train acc 0.7656\n",
      "Step 13000: train loss 0.4358, train acc 0.8594\n",
      "Step 13000: val loss 0.5387, val acc 0.7742\n",
      "Step 13100: train loss 0.4570, train acc 0.8594\n",
      "Step 13200: train loss 0.4208, train acc 0.8438\n",
      "Step 13300: train loss 0.4818, train acc 0.8281\n",
      "Step 13400: train loss 0.3618, train acc 0.9531\n",
      "Step 13500: train loss 0.4355, train acc 0.8594\n",
      "Step 13600: train loss 0.4570, train acc 0.8281\n",
      "Step 13700: train loss 0.4153, train acc 0.8438\n",
      "Step 13800: train loss 0.4133, train acc 0.8125\n",
      "Step 13900: train loss 0.3429, train acc 0.8438\n",
      "Step 14000: train loss 0.4452, train acc 0.8750\n",
      "Step 14000: val loss 0.4400, val acc 0.8471\n",
      "Step 14100: train loss 0.3783, train acc 0.8750\n",
      "Step 14200: train loss 0.4433, train acc 0.8594\n",
      "Step 14300: train loss 0.4238, train acc 0.8594\n",
      "Step 14400: train loss 0.4764, train acc 0.8438\n",
      "Step 14500: train loss 0.3508, train acc 0.8750\n",
      "Step 14600: train loss 0.4598, train acc 0.7656\n",
      "Step 14700: train loss 0.4049, train acc 0.8125\n",
      "Step 14800: train loss 0.4133, train acc 0.8594\n",
      "Step 14900: train loss 0.3106, train acc 0.9531\n",
      "Step 15000: train loss 0.4047, train acc 0.8594\n",
      "Step 15000: val loss 0.4123, val acc 0.8621\n",
      "Step 15100: train loss 0.4445, train acc 0.8438\n",
      "Step 15200: train loss 0.3330, train acc 0.8594\n",
      "Step 15300: train loss 0.3785, train acc 0.8438\n",
      "Step 15400: train loss 0.3478, train acc 0.8750\n",
      "Step 15500: train loss 0.2762, train acc 0.9219\n",
      "Step 15600: train loss 0.3101, train acc 0.9062\n",
      "Step 15700: train loss 0.3782, train acc 0.8594\n",
      "Step 15800: train loss 0.2785, train acc 0.9062\n",
      "Step 15900: train loss 0.2747, train acc 0.9062\n",
      "Step 16000: train loss 0.2914, train acc 0.9062\n",
      "Step 16000: val loss 0.3356, val acc 0.8995\n",
      "Step 16100: train loss 0.2498, train acc 0.9375\n",
      "Step 16200: train loss 0.5097, train acc 0.8281\n",
      "Step 16300: train loss 0.2859, train acc 0.9219\n",
      "Step 16400: train loss 0.2462, train acc 0.9219\n",
      "Step 16500: train loss 0.2501, train acc 0.9375\n",
      "Step 16600: train loss 0.2227, train acc 0.9375\n",
      "Step 16700: train loss 0.2133, train acc 0.9219\n",
      "Step 16800: train loss 0.2960, train acc 0.9062\n",
      "Step 16900: train loss 0.4061, train acc 0.8438\n",
      "Step 17000: train loss 0.2824, train acc 0.8750\n",
      "Step 17000: val loss 0.3087, val acc 0.8961\n",
      "Step 17100: train loss 0.2011, train acc 0.9375\n",
      "Step 17200: train loss 0.2804, train acc 0.8966\n",
      "Step 17300: train loss 0.2968, train acc 0.9062\n",
      "Step 17400: train loss 0.2396, train acc 0.9219\n",
      "Step 17500: train loss 0.2938, train acc 0.9219\n",
      "Step 17600: train loss 0.3934, train acc 0.9062\n",
      "Step 17700: train loss 0.3399, train acc 0.8594\n",
      "Step 17800: train loss 0.4363, train acc 0.8281\n",
      "Step 17900: train loss 0.1983, train acc 0.9375\n",
      "Step 18000: train loss 0.1388, train acc 0.9844\n",
      "Step 18000: val loss 0.2189, val acc 0.9409\n",
      "Step 18100: train loss 0.1741, train acc 0.9531\n",
      "Step 18200: train loss 0.1527, train acc 0.9375\n",
      "Step 18300: train loss 0.1923, train acc 0.9531\n",
      "Step 18400: train loss 0.1878, train acc 0.9531\n",
      "Step 18500: train loss 0.1338, train acc 1.0000\n",
      "Step 18600: train loss 0.1761, train acc 0.9531\n",
      "Step 18700: train loss 0.1359, train acc 0.9844\n",
      "Step 18800: train loss 0.1695, train acc 0.9531\n",
      "Step 18900: train loss 0.1238, train acc 0.9688\n",
      "Step 19000: train loss 0.1537, train acc 0.9688\n",
      "Step 19000: val loss 0.2112, val acc 0.9416\n",
      "Step 19100: train loss 0.1800, train acc 0.9375\n",
      "Step 19200: train loss 0.1595, train acc 0.9531\n",
      "Step 19300: train loss 0.1744, train acc 0.9219\n",
      "Step 19400: train loss 0.1243, train acc 0.9531\n",
      "Step 19500: train loss 0.0965, train acc 0.9844\n",
      "Step 19600: train loss 0.0899, train acc 0.9844\n",
      "Step 19700: train loss 0.1400, train acc 0.9219\n",
      "Step 19800: train loss 0.0902, train acc 0.9844\n",
      "Step 19900: train loss 0.1290, train acc 0.9531\n",
      "Step 20000: train loss 0.1874, train acc 0.9375\n",
      "Step 20000: val loss 0.2165, val acc 0.9260\n",
      "Step 20100: train loss 0.1211, train acc 0.9688\n",
      "Step 20200: train loss 0.3192, train acc 0.9531\n",
      "Step 20300: train loss 0.1994, train acc 0.8906\n",
      "Step 20400: train loss 0.1873, train acc 0.9531\n",
      "Step 20500: train loss 0.1436, train acc 0.9531\n",
      "Step 20600: train loss 0.1016, train acc 0.9844\n",
      "Step 20700: train loss 0.2247, train acc 0.9062\n",
      "Step 20800: train loss 0.2118, train acc 0.9531\n",
      "Step 20900: train loss 0.1246, train acc 0.9531\n",
      "Step 21000: train loss 0.1859, train acc 0.9219\n",
      "Step 21000: val loss 0.1389, val acc 0.9654\n",
      "Step 21100: train loss 0.0666, train acc 1.0000\n",
      "Step 21200: train loss 0.1633, train acc 0.9531\n",
      "Step 21300: train loss 0.1221, train acc 0.9531\n",
      "Step 21400: train loss 0.0634, train acc 1.0000\n",
      "Step 21500: train loss 0.1699, train acc 0.9375\n",
      "Step 21600: train loss 0.1666, train acc 0.9688\n",
      "Step 21700: train loss 0.0739, train acc 1.0000\n",
      "Step 21800: train loss 0.0716, train acc 0.9844\n",
      "Step 21900: train loss 0.1529, train acc 0.9375\n",
      "Step 22000: train loss 0.0700, train acc 1.0000\n",
      "Step 22000: val loss 0.1305, val acc 0.9620\n",
      "Step 22100: train loss 0.0680, train acc 1.0000\n",
      "Step 22200: train loss 0.0501, train acc 1.0000\n",
      "Step 22300: train loss 0.1870, train acc 0.9375\n",
      "Step 22400: train loss 0.2081, train acc 0.9219\n",
      "Step 22500: train loss 0.0858, train acc 0.9844\n",
      "Step 22600: train loss 0.0790, train acc 0.9844\n",
      "Step 22700: train loss 0.0970, train acc 0.9844\n",
      "Step 22800: train loss 0.0550, train acc 0.9844\n",
      "Step 22900: train loss 0.0555, train acc 0.9844\n",
      "Step 23000: train loss 0.0552, train acc 1.0000\n",
      "Step 23000: val loss 0.0914, val acc 0.9769\n",
      "Step 23100: train loss 0.0755, train acc 0.9688\n",
      "Step 23200: train loss 0.0354, train acc 0.9844\n",
      "Step 23300: train loss 0.1182, train acc 0.9688\n",
      "Step 23400: train loss 0.0654, train acc 1.0000\n",
      "Step 23500: train loss 0.0634, train acc 0.9844\n",
      "Step 23600: train loss 0.0804, train acc 0.9688\n",
      "Step 23700: train loss 0.0307, train acc 1.0000\n",
      "Step 23800: train loss 0.0874, train acc 0.9844\n",
      "Step 23900: train loss 0.1101, train acc 0.9688\n",
      "Step 24000: train loss 0.0480, train acc 0.9844\n",
      "Step 24000: val loss 0.0801, val acc 0.9817\n",
      "Step 24100: train loss 0.0741, train acc 0.9688\n",
      "Step 24200: train loss 0.0544, train acc 1.0000\n",
      "Step 24300: train loss 0.0590, train acc 1.0000\n",
      "Step 24400: train loss 0.0369, train acc 1.0000\n",
      "Step 24500: train loss 0.0598, train acc 0.9844\n",
      "Step 24600: train loss 0.0488, train acc 0.9844\n",
      "Step 24700: train loss 0.0464, train acc 1.0000\n",
      "Step 24800: train loss 0.0373, train acc 1.0000\n",
      "Step 24900: train loss 0.0394, train acc 1.0000\n",
      "Step 25000: train loss 0.0504, train acc 0.9844\n",
      "Step 25000: val loss 0.0823, val acc 0.9776\n",
      "Step 25100: train loss 0.0500, train acc 1.0000\n",
      "Step 25200: train loss 0.0616, train acc 0.9844\n",
      "Step 25300: train loss 0.0538, train acc 0.9688\n",
      "Step 25400: train loss 0.0419, train acc 1.0000\n",
      "Step 25500: train loss 0.0372, train acc 1.0000\n",
      "Step 25600: train loss 0.0595, train acc 0.9844\n",
      "Step 25700: train loss 0.0408, train acc 1.0000\n",
      "Step 25800: train loss 0.0404, train acc 0.9844\n",
      "Step 25900: train loss 0.0611, train acc 0.9688\n",
      "Step 26000: train loss 0.0609, train acc 0.9688\n",
      "Step 26000: val loss 0.0973, val acc 0.9749\n",
      "Step 26100: train loss 0.0376, train acc 1.0000\n",
      "Step 26200: train loss 0.0472, train acc 0.9844\n",
      "Step 26300: train loss 0.0504, train acc 0.9844\n",
      "Step 26400: train loss 0.0634, train acc 0.9844\n",
      "Step 26500: train loss 0.0307, train acc 1.0000\n",
      "Step 26600: train loss 0.0368, train acc 0.9844\n",
      "Step 26700: train loss 0.0587, train acc 0.9844\n",
      "Step 26800: train loss 0.0634, train acc 0.9844\n",
      "Step 26900: train loss 0.0459, train acc 0.9688\n",
      "Step 27000: train loss 0.0565, train acc 1.0000\n",
      "Step 27000: val loss 0.0659, val acc 0.9830\n",
      "Step 27100: train loss 0.0408, train acc 1.0000\n",
      "Step 27200: train loss 0.0497, train acc 0.9844\n",
      "Step 27300: train loss 0.0349, train acc 1.0000\n",
      "Step 27400: train loss 0.0198, train acc 1.0000\n",
      "Step 27500: train loss 0.0142, train acc 1.0000\n",
      "Step 27600: train loss 0.0580, train acc 0.9844\n",
      "Step 27700: train loss 0.0198, train acc 1.0000\n",
      "Step 27800: train loss 0.0269, train acc 1.0000\n",
      "Step 27900: train loss 0.0918, train acc 0.9844\n",
      "Step 28000: train loss 0.0303, train acc 1.0000\n",
      "Step 28000: val loss 0.0513, val acc 0.9830\n",
      "Step 28100: train loss 0.1123, train acc 0.9531\n",
      "Step 28200: train loss 0.0238, train acc 1.0000\n",
      "Step 28300: train loss 0.1463, train acc 0.9219\n",
      "Step 28400: train loss 0.0619, train acc 0.9844\n",
      "Step 28500: train loss 0.0234, train acc 1.0000\n",
      "Step 28600: train loss 0.1402, train acc 0.9531\n",
      "Step 28700: train loss 0.0135, train acc 1.0000\n",
      "Step 28800: train loss 0.0673, train acc 0.9844\n",
      "Step 28900: train loss 0.0132, train acc 1.0000\n",
      "Step 29000: train loss 0.0227, train acc 0.9844\n",
      "Step 29000: val loss 0.0536, val acc 0.9864\n",
      "Step 29100: train loss 0.2050, train acc 0.9531\n",
      "Step 29200: train loss 0.0396, train acc 0.9844\n",
      "Step 29300: train loss 0.0237, train acc 1.0000\n",
      "Step 29400: train loss 0.0370, train acc 0.9844\n",
      "Step 29500: train loss 0.0254, train acc 0.9844\n",
      "Step 29600: train loss 0.0161, train acc 1.0000\n",
      "Step 29700: train loss 0.0283, train acc 1.0000\n",
      "Step 29800: train loss 0.0314, train acc 0.9844\n",
      "Step 29900: train loss 0.0280, train acc 0.9844\n",
      "Step 30000: train loss 0.0785, train acc 0.9844\n",
      "Step 30000: val loss 0.0780, val acc 0.9708\n",
      "Step 30100: train loss 0.0210, train acc 1.0000\n",
      "Step 30200: train loss 0.0092, train acc 1.0000\n",
      "Step 30300: train loss 0.0349, train acc 0.9844\n",
      "Step 30400: train loss 0.0256, train acc 1.0000\n",
      "Step 30500: train loss 0.0544, train acc 0.9844\n",
      "Step 30600: train loss 0.0130, train acc 1.0000\n",
      "Step 30700: train loss 0.0138, train acc 1.0000\n",
      "Step 30800: train loss 0.0101, train acc 1.0000\n",
      "Step 30900: train loss 0.0086, train acc 1.0000\n",
      "Step 31000: train loss 0.0138, train acc 1.0000\n",
      "Step 31000: val loss 0.0483, val acc 0.9857\n",
      "Step 31100: train loss 0.0535, train acc 0.9844\n",
      "Step 31200: train loss 0.0197, train acc 1.0000\n",
      "Step 31300: train loss 0.0742, train acc 0.9688\n",
      "Step 31400: train loss 0.0081, train acc 1.0000\n",
      "Step 31500: train loss 0.0416, train acc 0.9688\n",
      "Step 31600: train loss 0.0823, train acc 0.9688\n",
      "Step 31700: train loss 0.0725, train acc 0.9844\n",
      "Step 31800: train loss 0.0114, train acc 1.0000\n",
      "Step 31900: train loss 0.0228, train acc 1.0000\n",
      "Step 32000: train loss 0.0492, train acc 0.9844\n",
      "Step 32000: val loss 0.0621, val acc 0.9864\n",
      "Step 32100: train loss 0.0082, train acc 1.0000\n",
      "Step 32200: train loss 0.0077, train acc 1.0000\n",
      "Step 32300: train loss 0.1013, train acc 0.9844\n",
      "Step 32400: train loss 0.0248, train acc 0.9844\n",
      "Step 32500: train loss 0.0250, train acc 1.0000\n",
      "Step 32600: train loss 0.0053, train acc 1.0000\n",
      "Step 32700: train loss 0.0379, train acc 0.9844\n",
      "Step 32800: train loss 0.0058, train acc 1.0000\n",
      "Step 32900: train loss 0.0077, train acc 1.0000\n",
      "Step 33000: train loss 0.0204, train acc 1.0000\n",
      "Step 33000: val loss 0.0286, val acc 0.9891\n",
      "Step 33100: train loss 0.0334, train acc 0.9844\n",
      "Step 33200: train loss 0.0149, train acc 1.0000\n",
      "Step 33300: train loss 0.0075, train acc 1.0000\n",
      "Step 33400: train loss 0.0054, train acc 1.0000\n",
      "Step 33500: train loss 0.0095, train acc 1.0000\n",
      "Step 33600: train loss 0.0119, train acc 1.0000\n",
      "Step 33700: train loss 0.0166, train acc 1.0000\n",
      "Step 33800: train loss 0.0065, train acc 1.0000\n",
      "Step 33900: train loss 0.0455, train acc 0.9844\n",
      "Step 34000: train loss 0.0137, train acc 1.0000\n",
      "Step 34000: val loss 0.0426, val acc 0.9932\n",
      "Step 34100: train loss 0.0087, train acc 1.0000\n",
      "Step 34200: train loss 0.0290, train acc 0.9844\n",
      "Step 34300: train loss 0.0041, train acc 1.0000\n",
      "Step 34400: train loss 0.0094, train acc 1.0000\n",
      "Step 34500: train loss 0.0164, train acc 1.0000\n",
      "Step 34600: train loss 0.0060, train acc 1.0000\n",
      "Step 34700: train loss 0.0144, train acc 1.0000\n",
      "Step 34800: train loss 0.0206, train acc 0.9844\n",
      "Step 34900: train loss 0.0164, train acc 1.0000\n",
      "Step 35000: train loss 0.0101, train acc 1.0000\n",
      "Step 35000: val loss 0.0260, val acc 0.9952\n",
      "Step 35100: train loss 0.0137, train acc 1.0000\n",
      "Step 35200: train loss 0.6534, train acc 0.9688\n",
      "Step 35300: train loss 0.0102, train acc 1.0000\n",
      "Step 35400: train loss 0.0146, train acc 1.0000\n",
      "Step 35500: train loss 0.0213, train acc 0.9844\n",
      "Step 35600: train loss 0.0031, train acc 1.0000\n",
      "Step 35700: train loss 0.0086, train acc 1.0000\n",
      "Step 35800: train loss 0.0054, train acc 1.0000\n",
      "Step 35900: train loss 0.0123, train acc 1.0000\n",
      "Step 36000: train loss 0.0117, train acc 1.0000\n",
      "Step 36000: val loss 0.0609, val acc 0.9830\n",
      "Step 36100: train loss 0.0200, train acc 1.0000\n",
      "Step 36200: train loss 0.0157, train acc 0.9844\n",
      "Step 36300: train loss 0.0747, train acc 0.9688\n",
      "Step 36400: train loss 0.0021, train acc 1.0000\n",
      "Step 36500: train loss 0.0070, train acc 1.0000\n",
      "Step 36600: train loss 0.0062, train acc 1.0000\n",
      "Step 36700: train loss 0.0064, train acc 1.0000\n",
      "Step 36800: train loss 0.0041, train acc 1.0000\n",
      "Step 36900: train loss 0.0301, train acc 0.9844\n",
      "Step 37000: train loss 0.0503, train acc 0.9688\n",
      "Step 37000: val loss 0.0354, val acc 0.9918\n",
      "Step 37100: train loss 0.0257, train acc 0.9844\n",
      "Step 37200: train loss 0.0049, train acc 1.0000\n",
      "Step 37300: train loss 0.0619, train acc 0.9844\n",
      "Step 37400: train loss 0.0052, train acc 1.0000\n",
      "Step 37500: train loss 0.0796, train acc 0.9688\n",
      "Step 37600: train loss 0.1450, train acc 0.9844\n",
      "Step 37700: train loss 0.0097, train acc 1.0000\n",
      "Step 37800: train loss 0.5319, train acc 0.9828\n",
      "Step 37900: train loss 0.0064, train acc 1.0000\n",
      "Step 38000: train loss 0.0065, train acc 1.0000\n",
      "Step 38000: val loss 0.0479, val acc 0.9871\n",
      "Step 38100: train loss 0.0076, train acc 1.0000\n",
      "Step 38200: train loss 0.0050, train acc 1.0000\n",
      "Step 38300: train loss 0.0063, train acc 1.0000\n",
      "Step 38400: train loss 0.0112, train acc 1.0000\n",
      "Step 38500: train loss 0.0028, train acc 1.0000\n",
      "Step 38600: train loss 0.0199, train acc 1.0000\n",
      "Step 38700: train loss 0.1012, train acc 0.9688\n",
      "Step 38800: train loss 0.0040, train acc 1.0000\n",
      "Step 38900: train loss 0.0141, train acc 1.0000\n",
      "Step 39000: train loss 0.0039, train acc 1.0000\n",
      "Step 39000: val loss 0.0271, val acc 0.9925\n",
      "Step 39100: train loss 0.0038, train acc 1.0000\n",
      "Step 39200: train loss 0.0040, train acc 1.0000\n",
      "Step 39300: train loss 0.0148, train acc 1.0000\n",
      "Step 39400: train loss 0.0679, train acc 0.9844\n",
      "Step 39500: train loss 0.0178, train acc 1.0000\n",
      "Step 39600: train loss 0.0715, train acc 0.9844\n",
      "Step 39700: train loss 0.1820, train acc 0.9844\n",
      "Step 39800: train loss 0.0143, train acc 1.0000\n",
      "Step 39900: train loss 0.0090, train acc 1.0000\n",
      "Step 40000: train loss 0.0349, train acc 0.9844\n",
      "Step 40000: val loss 0.0436, val acc 0.9891\n",
      "Step 40100: train loss 0.0039, train acc 1.0000\n",
      "Step 40200: train loss 0.0365, train acc 0.9844\n",
      "Step 40300: train loss 0.0201, train acc 1.0000\n",
      "Step 40400: train loss 0.0025, train acc 1.0000\n",
      "Step 40500: train loss 0.0013, train acc 1.0000\n",
      "Step 40600: train loss 0.0073, train acc 1.0000\n",
      "Step 40700: train loss 0.0012, train acc 1.0000\n",
      "Step 40800: train loss 0.0059, train acc 1.0000\n",
      "Step 40900: train loss 0.0071, train acc 1.0000\n",
      "Step 41000: train loss 0.0659, train acc 0.9844\n",
      "Step 41000: val loss 0.0463, val acc 0.9885\n",
      "Step 41100: train loss 0.0504, train acc 0.9688\n",
      "Step 41200: train loss 0.0018, train acc 1.0000\n",
      "Step 41300: train loss 0.0069, train acc 1.0000\n",
      "Step 41400: train loss 0.0275, train acc 0.9844\n",
      "Step 41500: train loss 0.0020, train acc 1.0000\n",
      "Step 41600: train loss 0.0618, train acc 0.9688\n",
      "Step 41700: train loss 0.0480, train acc 0.9844\n",
      "Step 41800: train loss 0.0055, train acc 1.0000\n",
      "Step 41900: train loss 0.0033, train acc 1.0000\n",
      "Step 42000: train loss 0.0410, train acc 0.9688\n",
      "Step 42000: val loss 0.0436, val acc 0.9851\n",
      "Step 42100: train loss 0.0038, train acc 1.0000\n",
      "Step 42200: train loss 0.0040, train acc 1.0000\n",
      "Step 42300: train loss 0.0026, train acc 1.0000\n",
      "Step 42400: train loss 0.0144, train acc 0.9844\n",
      "Step 42500: train loss 0.0174, train acc 0.9844\n",
      "Step 42600: train loss 0.0533, train acc 0.9844\n",
      "Step 42700: train loss 0.0040, train acc 1.0000\n",
      "Step 42800: train loss 0.0016, train acc 1.0000\n",
      "Step 42900: train loss 0.0040, train acc 1.0000\n",
      "Step 43000: train loss 0.0025, train acc 1.0000\n",
      "Step 43000: val loss 0.0232, val acc 0.9932\n",
      "Step 43100: train loss 0.0060, train acc 1.0000\n",
      "Step 43200: train loss 0.0007, train acc 1.0000\n",
      "Step 43300: train loss 0.0240, train acc 0.9844\n",
      "Step 43400: train loss 0.0047, train acc 1.0000\n",
      "Step 43500: train loss 0.0178, train acc 1.0000\n",
      "Step 43600: train loss 0.0036, train acc 1.0000\n",
      "Step 43700: train loss 0.0175, train acc 1.0000\n",
      "Step 43800: train loss 0.0246, train acc 0.9844\n",
      "Step 43900: train loss 0.0050, train acc 1.0000\n",
      "Step 44000: train loss 0.0020, train acc 1.0000\n",
      "Step 44000: val loss 0.0291, val acc 0.9912\n",
      "Step 44100: train loss 0.0041, train acc 1.0000\n",
      "Step 44200: train loss 0.0133, train acc 1.0000\n",
      "Step 44300: train loss 0.0114, train acc 0.9844\n",
      "Step 44400: train loss 0.0010, train acc 1.0000\n",
      "Step 44500: train loss 0.0124, train acc 1.0000\n",
      "Step 44600: train loss 0.0056, train acc 1.0000\n",
      "Step 44700: train loss 0.0061, train acc 1.0000\n",
      "Step 44800: train loss 0.0912, train acc 0.9531\n",
      "Step 44900: train loss 0.0053, train acc 1.0000\n",
      "Step 45000: train loss 0.0027, train acc 1.0000\n",
      "Step 45000: val loss 0.0392, val acc 0.9871\n",
      "Step 45100: train loss 0.0029, train acc 1.0000\n",
      "Step 45200: train loss 0.0016, train acc 1.0000\n",
      "Step 45300: train loss 0.1654, train acc 0.9688\n",
      "Step 45400: train loss 0.0144, train acc 0.9844\n",
      "Step 45500: train loss 0.0021, train acc 1.0000\n",
      "Step 45600: train loss 0.0045, train acc 1.0000\n",
      "Step 45700: train loss 0.0019, train acc 1.0000\n",
      "Step 45800: train loss 0.0244, train acc 0.9844\n",
      "Step 45900: train loss 0.0020, train acc 1.0000\n",
      "Step 46000: train loss 0.0352, train acc 0.9844\n",
      "Step 46000: val loss 0.0208, val acc 0.9918\n",
      "Step 46100: train loss 0.0047, train acc 1.0000\n",
      "Step 46200: train loss 0.0014, train acc 1.0000\n",
      "Step 46300: train loss 0.0041, train acc 1.0000\n",
      "Step 46400: train loss 0.0434, train acc 0.9844\n",
      "Step 46500: train loss 0.0356, train acc 0.9844\n",
      "Step 46600: train loss 0.0061, train acc 1.0000\n",
      "Step 46700: train loss 0.0119, train acc 1.0000\n",
      "Step 46800: train loss 0.0342, train acc 0.9844\n",
      "Step 46900: train loss 0.0193, train acc 0.9844\n",
      "Step 47000: train loss 0.0013, train acc 1.0000\n",
      "Step 47000: val loss 0.0208, val acc 0.9946\n",
      "Step 47100: train loss 0.0031, train acc 1.0000\n",
      "Step 47200: train loss 0.0118, train acc 0.9844\n",
      "Step 47300: train loss 0.0014, train acc 1.0000\n",
      "Step 47400: train loss 0.0138, train acc 0.9844\n",
      "Step 47500: train loss 0.0029, train acc 1.0000\n",
      "Step 47600: train loss 0.0020, train acc 1.0000\n",
      "Step 47700: train loss 0.0083, train acc 1.0000\n",
      "Step 47800: train loss 0.0031, train acc 1.0000\n",
      "Step 47900: train loss 0.0037, train acc 1.0000\n",
      "Step 48000: train loss 0.0022, train acc 1.0000\n",
      "Step 48000: val loss 0.0194, val acc 0.9946\n",
      "Step 48100: train loss 0.0035, train acc 1.0000\n",
      "Step 48200: train loss 0.0141, train acc 1.0000\n",
      "Step 48300: train loss 0.0037, train acc 1.0000\n",
      "Step 48400: train loss 0.0067, train acc 1.0000\n",
      "Step 48500: train loss 0.0072, train acc 1.0000\n",
      "Step 48600: train loss 0.0054, train acc 1.0000\n",
      "Step 48700: train loss 0.0649, train acc 0.9844\n",
      "Step 48800: train loss 0.0016, train acc 1.0000\n",
      "Step 48900: train loss 0.0016, train acc 1.0000\n",
      "Step 49000: train loss 0.0596, train acc 0.9531\n",
      "Step 49000: val loss 0.0320, val acc 0.9939\n",
      "Step 49100: train loss 0.0026, train acc 1.0000\n",
      "Step 49200: train loss 0.0019, train acc 1.0000\n",
      "Step 49300: train loss 0.0098, train acc 1.0000\n",
      "Step 49400: train loss 0.0015, train acc 1.0000\n",
      "Step 49500: train loss 0.0871, train acc 0.9688\n",
      "Step 49600: train loss 0.0215, train acc 0.9844\n",
      "Step 49700: train loss 0.1095, train acc 0.9844\n",
      "Step 49800: train loss 0.0017, train acc 1.0000\n",
      "Step 49900: train loss 0.0019, train acc 1.0000\n",
      "Step 50000: train loss 0.0017, train acc 1.0000\n",
      "Step 50000: val loss 0.0518, val acc 0.9898\n",
      "Step 50100: train loss 0.0129, train acc 0.9844\n",
      "Step 50200: train loss 0.0045, train acc 1.0000\n",
      "Step 50300: train loss 0.0306, train acc 0.9844\n",
      "Step 50400: train loss 0.0060, train acc 1.0000\n",
      "Step 50500: train loss 0.0084, train acc 1.0000\n",
      "Step 50600: train loss 0.0220, train acc 0.9844\n",
      "Step 50700: train loss 0.0009, train acc 1.0000\n",
      "Step 50800: train loss 0.0011, train acc 1.0000\n",
      "Step 50900: train loss 0.0005, train acc 1.0000\n",
      "Step 51000: train loss 0.0035, train acc 1.0000\n",
      "Step 51000: val loss 0.0093, val acc 0.9980\n",
      "Step 51100: train loss 0.0021, train acc 1.0000\n",
      "Step 51200: train loss 0.0211, train acc 0.9844\n",
      "Step 51300: train loss 0.0010, train acc 1.0000\n",
      "Step 51400: train loss 0.0261, train acc 0.9844\n",
      "Step 51500: train loss 0.0358, train acc 0.9844\n",
      "Step 51600: train loss 0.0034, train acc 1.0000\n",
      "Step 51700: train loss 0.0005, train acc 1.0000\n",
      "Step 51800: train loss 0.0090, train acc 1.0000\n",
      "Step 51900: train loss 0.0671, train acc 0.9688\n",
      "Step 52000: train loss 0.0020, train acc 1.0000\n",
      "Step 52000: val loss 0.0221, val acc 0.9939\n",
      "Step 52100: train loss 0.0011, train acc 1.0000\n",
      "Step 52200: train loss 0.0686, train acc 0.9688\n",
      "Step 52300: train loss 0.0029, train acc 1.0000\n",
      "Step 52400: train loss 0.0340, train acc 0.9844\n",
      "Step 52500: train loss 0.0018, train acc 1.0000\n",
      "Step 52600: train loss 0.0015, train acc 1.0000\n",
      "Step 52700: train loss 0.0083, train acc 1.0000\n",
      "Step 52800: train loss 0.0029, train acc 1.0000\n",
      "Step 52900: train loss 0.0055, train acc 1.0000\n",
      "Step 53000: train loss 0.0031, train acc 1.0000\n",
      "Step 53000: val loss 0.0280, val acc 0.9918\n",
      "Step 53100: train loss 0.0025, train acc 1.0000\n",
      "Step 53200: train loss 0.0008, train acc 1.0000\n",
      "Step 53300: train loss 0.0007, train acc 1.0000\n",
      "Step 53400: train loss 0.0064, train acc 1.0000\n",
      "Step 53500: train loss 0.0057, train acc 1.0000\n",
      "Step 53600: train loss 0.0488, train acc 0.9844\n",
      "Step 53700: train loss 0.0013, train acc 1.0000\n",
      "Step 53800: train loss 0.0011, train acc 1.0000\n",
      "Step 53900: train loss 0.0360, train acc 0.9844\n",
      "Step 54000: train loss 0.0081, train acc 1.0000\n",
      "Step 54000: val loss 0.0234, val acc 0.9918\n",
      "Step 54100: train loss 0.0037, train acc 1.0000\n",
      "Step 54200: train loss 0.0015, train acc 1.0000\n",
      "Step 54300: train loss 0.0346, train acc 0.9844\n",
      "Step 54400: train loss 0.0019, train acc 1.0000\n",
      "Step 54500: train loss 0.0176, train acc 0.9844\n",
      "Step 54600: train loss 0.0023, train acc 1.0000\n",
      "Step 54700: train loss 0.0070, train acc 1.0000\n",
      "Step 54800: train loss 0.0029, train acc 1.0000\n",
      "Step 54900: train loss 0.0127, train acc 0.9844\n",
      "Step 55000: train loss 0.0066, train acc 1.0000\n",
      "Step 55000: val loss 0.0194, val acc 0.9939\n",
      "Step 55100: train loss 0.0053, train acc 1.0000\n",
      "Step 55200: train loss 0.0173, train acc 0.9844\n",
      "Step 55300: train loss 0.0011, train acc 1.0000\n",
      "Step 55400: train loss 0.0029, train acc 1.0000\n",
      "Step 55500: train loss 0.0200, train acc 0.9844\n",
      "Step 55600: train loss 0.0064, train acc 1.0000\n",
      "Step 55700: train loss 0.0020, train acc 1.0000\n",
      "Step 55800: train loss 0.0014, train acc 1.0000\n",
      "Step 55900: train loss 0.0279, train acc 0.9844\n",
      "Step 56000: train loss 0.0008, train acc 1.0000\n",
      "Step 56000: val loss 0.0152, val acc 0.9959\n",
      "Step 56100: train loss 0.0005, train acc 1.0000\n",
      "Step 56200: train loss 0.0044, train acc 1.0000\n",
      "Step 56300: train loss 0.0003, train acc 1.0000\n",
      "Step 56400: train loss 0.0016, train acc 1.0000\n",
      "Step 56500: train loss 0.0213, train acc 0.9844\n",
      "Step 56600: train loss 0.0006, train acc 1.0000\n",
      "Step 56700: train loss 0.0350, train acc 0.9844\n",
      "Step 56800: train loss 0.0017, train acc 1.0000\n",
      "Step 56900: train loss 0.0005, train acc 1.0000\n",
      "Step 57000: train loss 0.0708, train acc 0.9844\n",
      "Step 57000: val loss 0.0247, val acc 0.9946\n",
      "Step 57100: train loss 0.0065, train acc 1.0000\n",
      "Step 57200: train loss 0.0100, train acc 1.0000\n",
      "Step 57300: train loss 0.0003, train acc 1.0000\n",
      "Step 57400: train loss 0.0548, train acc 0.9844\n",
      "Step 57500: train loss 0.0057, train acc 1.0000\n",
      "Step 57600: train loss 0.0058, train acc 1.0000\n",
      "Step 57700: train loss 0.0020, train acc 1.0000\n",
      "Step 57800: train loss 0.0007, train acc 1.0000\n",
      "Step 57900: train loss 0.0114, train acc 1.0000\n",
      "Step 58000: train loss 0.0008, train acc 1.0000\n",
      "Step 58000: val loss 0.0602, val acc 0.9918\n",
      "Step 58100: train loss 0.0336, train acc 0.9844\n",
      "Step 58200: train loss 0.0053, train acc 1.0000\n",
      "Step 58300: train loss 0.0052, train acc 1.0000\n",
      "Step 58400: train loss 0.0026, train acc 1.0000\n",
      "Step 58500: train loss 0.0128, train acc 1.0000\n",
      "Step 58600: train loss 0.0029, train acc 1.0000\n",
      "Step 58700: train loss 0.0261, train acc 1.0000\n",
      "Step 58800: train loss 0.0348, train acc 0.9844\n",
      "Step 58900: train loss 0.0211, train acc 0.9844\n",
      "Step 59000: train loss 0.0822, train acc 0.9688\n",
      "Step 59000: val loss 0.0309, val acc 0.9912\n",
      "Step 59100: train loss 0.0062, train acc 1.0000\n",
      "Step 59200: train loss 0.0007, train acc 1.0000\n",
      "Step 59300: train loss 0.0009, train acc 1.0000\n",
      "Step 59400: train loss 0.0005, train acc 1.0000\n",
      "Step 59500: train loss 0.0017, train acc 1.0000\n",
      "Step 59600: train loss 0.0002, train acc 1.0000\n",
      "Step 59700: train loss 0.0379, train acc 0.9844\n",
      "Step 59800: train loss 0.0059, train acc 1.0000\n",
      "Step 59900: train loss 0.0003, train acc 1.0000\n",
      "Step 60000: train loss 0.0016, train acc 1.0000\n",
      "Step 60000: val loss 0.0261, val acc 0.9925\n",
      "Step 60100: train loss 0.0124, train acc 0.9844\n",
      "Step 60200: train loss 0.0077, train acc 1.0000\n",
      "Step 60300: train loss 0.0028, train acc 1.0000\n",
      "Step 60400: train loss 0.0076, train acc 1.0000\n",
      "Step 60500: train loss 0.0502, train acc 0.9531\n",
      "Step 60600: train loss 0.0008, train acc 1.0000\n",
      "Step 60700: train loss 0.0112, train acc 1.0000\n",
      "Step 60800: train loss 0.0014, train acc 1.0000\n",
      "Step 60900: train loss 0.0018, train acc 1.0000\n",
      "Step 61000: train loss 0.0013, train acc 1.0000\n",
      "Step 61000: val loss 0.0293, val acc 0.9939\n",
      "Step 61100: train loss 0.0057, train acc 1.0000\n",
      "Step 61200: train loss 0.0065, train acc 1.0000\n",
      "Step 61300: train loss 0.0114, train acc 1.0000\n",
      "Step 61400: train loss 0.0065, train acc 1.0000\n",
      "Step 61500: train loss 0.0016, train acc 1.0000\n",
      "Step 61600: train loss 0.0023, train acc 1.0000\n",
      "Step 61700: train loss 0.0082, train acc 1.0000\n",
      "Step 61800: train loss 0.0119, train acc 0.9844\n",
      "Step 61900: train loss 0.0010, train acc 1.0000\n",
      "Step 62000: train loss 0.0010, train acc 1.0000\n",
      "Step 62000: val loss 0.0463, val acc 0.9905\n",
      "Step 62100: train loss 0.0165, train acc 0.9844\n",
      "Step 62200: train loss 0.2234, train acc 0.9688\n",
      "Step 62300: train loss 0.0008, train acc 1.0000\n",
      "Step 62400: train loss 0.0005, train acc 1.0000\n",
      "Step 62500: train loss 0.0068, train acc 1.0000\n",
      "Step 62600: train loss 0.0055, train acc 1.0000\n",
      "Step 62700: train loss 0.0131, train acc 1.0000\n",
      "Step 62800: train loss 0.0221, train acc 0.9844\n",
      "Step 62900: train loss 0.0320, train acc 0.9844\n",
      "Step 63000: train loss 0.0011, train acc 1.0000\n",
      "Step 63000: val loss 0.0223, val acc 0.9918\n",
      "Step 63100: train loss 0.0045, train acc 1.0000\n",
      "Step 63200: train loss 0.0014, train acc 1.0000\n",
      "Step 63300: train loss 0.0025, train acc 1.0000\n",
      "Step 63400: train loss 0.0023, train acc 1.0000\n",
      "Step 63500: train loss 0.0036, train acc 1.0000\n",
      "Step 63600: train loss 0.0035, train acc 1.0000\n",
      "Step 63700: train loss 0.0005, train acc 1.0000\n",
      "Step 63800: train loss 0.0793, train acc 0.9844\n",
      "Step 63900: train loss 0.0021, train acc 1.0000\n",
      "Step 64000: train loss 0.0013, train acc 1.0000\n",
      "Step 64000: val loss 0.0265, val acc 0.9912\n",
      "Step 64100: train loss 0.0091, train acc 1.0000\n",
      "Step 64200: train loss 0.0020, train acc 1.0000\n",
      "Step 64300: train loss 0.0030, train acc 1.0000\n",
      "Step 64400: train loss 0.0125, train acc 1.0000\n",
      "Step 64500: train loss 0.0008, train acc 1.0000\n",
      "Step 64600: train loss 0.2659, train acc 0.9844\n",
      "Step 64700: train loss 0.0018, train acc 1.0000\n",
      "Step 64800: train loss 0.0113, train acc 1.0000\n",
      "Step 64900: train loss 0.0662, train acc 0.9844\n",
      "Step 65000: train loss 0.0104, train acc 1.0000\n",
      "Step 65000: val loss 0.0360, val acc 0.9918\n",
      "Step 65100: train loss 0.0023, train acc 1.0000\n",
      "Step 65200: train loss 0.0039, train acc 1.0000\n",
      "Step 65300: train loss 0.0017, train acc 1.0000\n",
      "Step 65400: train loss 0.0080, train acc 1.0000\n",
      "Step 65500: train loss 0.0016, train acc 1.0000\n",
      "Step 65600: train loss 0.0063, train acc 1.0000\n",
      "Step 65700: train loss 0.0004, train acc 1.0000\n",
      "Step 65800: train loss 0.0393, train acc 0.9844\n",
      "Step 65900: train loss 0.0022, train acc 1.0000\n",
      "Step 66000: train loss 0.0012, train acc 1.0000\n",
      "Step 66000: val loss 0.0208, val acc 0.9952\n",
      "Step 66100: train loss 0.0010, train acc 1.0000\n",
      "Step 66200: train loss 0.0020, train acc 1.0000\n",
      "Step 66300: train loss 0.0133, train acc 1.0000\n",
      "Step 66400: train loss 0.0594, train acc 0.9688\n",
      "Step 66500: train loss 0.0002, train acc 1.0000\n",
      "Step 66600: train loss 0.0006, train acc 1.0000\n",
      "Step 66700: train loss 0.0003, train acc 1.0000\n",
      "Step 66800: train loss 0.0595, train acc 0.9688\n",
      "Step 66900: train loss 0.0265, train acc 0.9844\n",
      "Step 67000: train loss 0.0057, train acc 1.0000\n",
      "Step 67000: val loss 0.0247, val acc 0.9932\n",
      "Step 67100: train loss 0.0270, train acc 0.9844\n",
      "Step 67200: train loss 0.0076, train acc 1.0000\n",
      "Step 67300: train loss 0.0005, train acc 1.0000\n",
      "Step 67400: train loss 0.0230, train acc 0.9688\n",
      "Step 67500: train loss 0.0348, train acc 0.9844\n",
      "Step 67600: train loss 0.0010, train acc 1.0000\n",
      "Step 67700: train loss 0.0218, train acc 0.9844\n",
      "Step 67800: train loss 0.0005, train acc 1.0000\n",
      "Step 67900: train loss 0.0004, train acc 1.0000\n",
      "Step 68000: train loss 0.0004, train acc 1.0000\n",
      "Step 68000: val loss 0.0721, val acc 0.9803\n",
      "Step 68100: train loss 0.0356, train acc 0.9844\n",
      "Step 68200: train loss 0.0237, train acc 0.9844\n",
      "Step 68300: train loss 0.0009, train acc 1.0000\n",
      "Step 68400: train loss 0.0033, train acc 1.0000\n",
      "Step 68500: train loss 0.0019, train acc 1.0000\n",
      "Step 68600: train loss 0.0006, train acc 1.0000\n",
      "Step 68700: train loss 0.0029, train acc 1.0000\n",
      "Step 68800: train loss 0.0175, train acc 1.0000\n",
      "Step 68900: train loss 0.0065, train acc 1.0000\n",
      "Step 69000: train loss 0.0005, train acc 1.0000\n",
      "Step 69000: val loss 0.0238, val acc 0.9932\n",
      "Step 69100: train loss 0.0025, train acc 1.0000\n",
      "Step 69200: train loss 0.0994, train acc 0.9688\n",
      "Step 69300: train loss 0.0065, train acc 1.0000\n",
      "Step 69400: train loss 0.0238, train acc 0.9844\n",
      "Step 69500: train loss 0.0830, train acc 0.9844\n",
      "Step 69600: train loss 0.0498, train acc 0.9844\n",
      "Step 69700: train loss 0.1809, train acc 0.9844\n",
      "Step 69800: train loss 0.0121, train acc 0.9844\n",
      "Step 69900: train loss 0.0134, train acc 0.9844\n",
      "Step 70000: train loss 0.0060, train acc 1.0000\n",
      "Step 70000: val loss 0.0155, val acc 0.9959\n",
      "Step 70100: train loss 0.0956, train acc 0.9844\n",
      "Step 70200: train loss 0.0368, train acc 0.9844\n",
      "Step 70300: train loss 0.0010, train acc 1.0000\n",
      "Step 70400: train loss 0.0065, train acc 1.0000\n",
      "Step 70500: train loss 0.0646, train acc 0.9844\n",
      "Step 70600: train loss 0.0329, train acc 0.9844\n",
      "Step 70700: train loss 0.0682, train acc 0.9844\n",
      "Step 70800: train loss 0.0415, train acc 0.9844\n",
      "Step 70900: train loss 0.0012, train acc 1.0000\n",
      "Step 71000: train loss 0.0050, train acc 1.0000\n",
      "Step 71000: val loss 0.0261, val acc 0.9932\n",
      "Step 71100: train loss 0.0033, train acc 1.0000\n",
      "Step 71200: train loss 0.0015, train acc 1.0000\n",
      "Step 71300: train loss 0.0020, train acc 1.0000\n",
      "Step 71400: train loss 0.0035, train acc 1.0000\n",
      "Step 71500: train loss 0.0029, train acc 1.0000\n",
      "Step 71600: train loss 0.0130, train acc 0.9844\n",
      "Step 71700: train loss 0.0211, train acc 0.9844\n",
      "Step 71800: train loss 0.0028, train acc 1.0000\n",
      "Step 71900: train loss 0.0453, train acc 0.9844\n",
      "Step 72000: train loss 0.0465, train acc 0.9844\n",
      "Step 72000: val loss 0.0130, val acc 0.9952\n",
      "Step 72100: train loss 0.0003, train acc 1.0000\n",
      "Step 72200: train loss 0.0005, train acc 1.0000\n",
      "Step 72300: train loss 0.0110, train acc 1.0000\n",
      "Step 72400: train loss 0.0015, train acc 1.0000\n",
      "Step 72500: train loss 0.0008, train acc 1.0000\n",
      "Step 72600: train loss 0.1001, train acc 0.9844\n",
      "Step 72700: train loss 0.0873, train acc 0.9688\n",
      "Step 72800: train loss 0.0006, train acc 1.0000\n",
      "Step 72900: train loss 0.0021, train acc 1.0000\n",
      "Step 73000: train loss 0.0024, train acc 1.0000\n",
      "Step 73000: val loss 0.0211, val acc 0.9959\n",
      "Step 73100: train loss 0.0017, train acc 1.0000\n",
      "Step 73200: train loss 0.0006, train acc 1.0000\n",
      "Step 73300: train loss 0.0021, train acc 1.0000\n",
      "Step 73400: train loss 0.0019, train acc 1.0000\n",
      "Step 73500: train loss 0.0021, train acc 1.0000\n",
      "Step 73600: train loss 0.0008, train acc 1.0000\n",
      "Step 73700: train loss 0.0439, train acc 0.9844\n",
      "Step 73800: train loss 0.0100, train acc 1.0000\n",
      "Step 73900: train loss 0.0007, train acc 1.0000\n",
      "Step 74000: train loss 0.1342, train acc 0.9844\n",
      "Step 74000: val loss 0.0157, val acc 0.9952\n",
      "Step 74100: train loss 0.0005, train acc 1.0000\n",
      "Step 74200: train loss 0.0019, train acc 1.0000\n",
      "Step 74300: train loss 0.0029, train acc 1.0000\n",
      "Step 74400: train loss 0.0044, train acc 1.0000\n",
      "Step 74500: train loss 0.0043, train acc 1.0000\n",
      "Step 74600: train loss 0.0021, train acc 1.0000\n",
      "Step 74700: train loss 0.0054, train acc 1.0000\n",
      "Step 74800: train loss 0.0390, train acc 0.9844\n",
      "Step 74900: train loss 0.0043, train acc 1.0000\n",
      "Step 75000: train loss 0.0008, train acc 1.0000\n",
      "Step 75000: val loss 0.0130, val acc 0.9946\n",
      "Step 75100: train loss 0.0053, train acc 1.0000\n",
      "Step 75200: train loss 0.0013, train acc 1.0000\n",
      "Step 75300: train loss 0.0534, train acc 0.9844\n",
      "Step 75400: train loss 0.0000, train acc 1.0000\n",
      "Step 75500: train loss 0.0007, train acc 1.0000\n",
      "Step 75600: train loss 0.0013, train acc 1.0000\n",
      "Step 75700: train loss 0.0005, train acc 1.0000\n",
      "Step 75800: train loss 0.0252, train acc 0.9844\n",
      "Step 75900: train loss 0.0005, train acc 1.0000\n",
      "Step 76000: train loss 0.1633, train acc 0.9375\n",
      "Step 76000: val loss 0.0458, val acc 0.9878\n",
      "Step 76100: train loss 0.0005, train acc 1.0000\n",
      "Step 76200: train loss 0.0013, train acc 1.0000\n",
      "Step 76300: train loss 0.0041, train acc 1.0000\n",
      "Step 76400: train loss 0.0488, train acc 0.9844\n",
      "Step 76500: train loss 0.0112, train acc 0.9844\n",
      "Step 76600: train loss 0.0237, train acc 0.9844\n",
      "Step 76700: train loss 0.0012, train acc 1.0000\n",
      "Step 76800: train loss 0.0001, train acc 1.0000\n",
      "Step 76900: train loss 0.0009, train acc 1.0000\n",
      "Step 77000: train loss 0.0133, train acc 0.9844\n",
      "Step 77000: val loss 0.0474, val acc 0.9891\n",
      "Step 77100: train loss 0.0061, train acc 1.0000\n",
      "Step 77200: train loss 0.0060, train acc 1.0000\n",
      "Step 77300: train loss 0.3909, train acc 0.9531\n",
      "Step 77400: train loss 0.0077, train acc 1.0000\n",
      "Step 77500: train loss 0.0003, train acc 1.0000\n",
      "Step 77600: train loss 0.0005, train acc 1.0000\n",
      "Step 77700: train loss 0.0002, train acc 1.0000\n",
      "Step 77800: train loss 0.0008, train acc 1.0000\n",
      "Step 77900: train loss 0.0022, train acc 1.0000\n",
      "Step 78000: train loss 0.0116, train acc 1.0000\n",
      "Step 78000: val loss 0.0199, val acc 0.9959\n",
      "Step 78100: train loss 0.0186, train acc 0.9844\n",
      "Step 78200: train loss 0.0003, train acc 1.0000\n",
      "Step 78300: train loss 0.0004, train acc 1.0000\n",
      "Step 78400: train loss 0.2787, train acc 0.9844\n",
      "Step 78500: train loss 0.0023, train acc 1.0000\n",
      "Step 78600: train loss 0.0163, train acc 0.9844\n",
      "Step 78700: train loss 0.0001, train acc 1.0000\n",
      "Step 78800: train loss 0.0034, train acc 1.0000\n",
      "Step 78900: train loss 0.0029, train acc 1.0000\n",
      "Step 79000: train loss 0.0013, train acc 1.0000\n",
      "Step 79000: val loss 0.0215, val acc 0.9946\n",
      "Step 79100: train loss 0.0487, train acc 0.9844\n",
      "Step 79200: train loss 0.1043, train acc 0.9844\n",
      "Step 79300: train loss 0.0110, train acc 0.9844\n",
      "Step 79400: train loss 0.0026, train acc 1.0000\n",
      "Step 79500: train loss 0.0022, train acc 1.0000\n",
      "Step 79600: train loss 0.0783, train acc 0.9844\n",
      "Step 79700: train loss 0.0032, train acc 1.0000\n",
      "Step 79800: train loss 0.0006, train acc 1.0000\n",
      "Step 79900: train loss 0.0004, train acc 1.0000\n",
      "Step 80000: train loss 0.0042, train acc 1.0000\n",
      "Step 80000: val loss 0.0130, val acc 0.9966\n",
      "Step 80100: train loss 0.0034, train acc 1.0000\n",
      "Step 80200: train loss 0.0651, train acc 0.9844\n",
      "Step 80300: train loss 0.0016, train acc 1.0000\n",
      "Step 80400: train loss 0.0006, train acc 1.0000\n",
      "Step 80500: train loss 0.0052, train acc 1.0000\n",
      "Step 80600: train loss 0.0548, train acc 0.9688\n",
      "Step 80700: train loss 0.0010, train acc 1.0000\n",
      "Step 80800: train loss 0.0028, train acc 1.0000\n",
      "Step 80900: train loss 0.0109, train acc 1.0000\n",
      "Step 81000: train loss 0.0006, train acc 1.0000\n",
      "Step 81000: val loss 0.0196, val acc 0.9952\n",
      "Step 81100: train loss 0.0005, train acc 1.0000\n",
      "Step 81200: train loss 0.0071, train acc 1.0000\n",
      "Step 81300: train loss 0.0004, train acc 1.0000\n",
      "Step 81400: train loss 0.0004, train acc 1.0000\n",
      "Step 81500: train loss 0.0022, train acc 1.0000\n",
      "Step 81600: train loss 0.0487, train acc 0.9844\n",
      "Step 81700: train loss 0.0265, train acc 0.9844\n",
      "Step 81800: train loss 0.0014, train acc 1.0000\n",
      "Step 81900: train loss 0.0031, train acc 1.0000\n",
      "Step 82000: train loss 0.0057, train acc 1.0000\n",
      "Step 82000: val loss 0.0510, val acc 0.9898\n",
      "Step 82100: train loss 0.0027, train acc 1.0000\n",
      "Step 82200: train loss 0.0005, train acc 1.0000\n",
      "Step 82300: train loss 0.0056, train acc 1.0000\n",
      "Step 82400: train loss 0.0009, train acc 1.0000\n",
      "Step 82500: train loss 0.0004, train acc 1.0000\n",
      "Step 82600: train loss 0.0002, train acc 1.0000\n",
      "Step 82700: train loss 0.0023, train acc 1.0000\n",
      "Step 82800: train loss 0.0042, train acc 1.0000\n",
      "Step 82900: train loss 0.0075, train acc 1.0000\n",
      "Step 83000: train loss 0.0005, train acc 1.0000\n",
      "Step 83000: val loss 0.0402, val acc 0.9918\n",
      "Step 83100: train loss 0.0061, train acc 1.0000\n",
      "Step 83200: train loss 0.0033, train acc 1.0000\n",
      "Step 83300: train loss 0.0236, train acc 0.9844\n",
      "Step 83400: train loss 0.0031, train acc 1.0000\n",
      "Step 83500: train loss 0.0023, train acc 1.0000\n",
      "Step 83600: train loss 0.0011, train acc 1.0000\n",
      "Step 83700: train loss 0.0345, train acc 0.9844\n",
      "Step 83800: train loss 0.0562, train acc 0.9844\n",
      "Step 83900: train loss 0.0120, train acc 1.0000\n",
      "Step 84000: train loss 0.0496, train acc 0.9844\n",
      "Step 84000: val loss 0.0304, val acc 0.9891\n",
      "Step 84100: train loss 0.0087, train acc 1.0000\n",
      "Step 84200: train loss 0.0032, train acc 1.0000\n",
      "Step 84300: train loss 0.0118, train acc 1.0000\n",
      "Step 84400: train loss 0.0011, train acc 1.0000\n",
      "Step 84500: train loss 0.0077, train acc 1.0000\n",
      "Step 84600: train loss 0.0209, train acc 0.9844\n",
      "Step 84700: train loss 0.0015, train acc 1.0000\n",
      "Step 84800: train loss 0.0326, train acc 0.9844\n",
      "Step 84900: train loss 0.0352, train acc 0.9844\n",
      "Step 85000: train loss 0.0036, train acc 1.0000\n",
      "Step 85000: val loss 0.0178, val acc 0.9966\n",
      "Step 85100: train loss 0.0033, train acc 1.0000\n",
      "Step 85200: train loss 0.0006, train acc 1.0000\n",
      "Step 85300: train loss 0.0002, train acc 1.0000\n",
      "Step 85400: train loss 0.0012, train acc 1.0000\n",
      "Step 85500: train loss 0.0058, train acc 1.0000\n",
      "Step 85600: train loss 0.0129, train acc 0.9844\n",
      "Step 85700: train loss 0.0119, train acc 0.9844\n",
      "Step 85800: train loss 0.0335, train acc 0.9844\n",
      "Step 85900: train loss 0.0432, train acc 0.9844\n",
      "Step 86000: train loss 0.0160, train acc 0.9844\n",
      "Step 86000: val loss 0.1093, val acc 0.9905\n",
      "Step 86100: train loss 0.2741, train acc 0.9688\n",
      "Step 86200: train loss 0.0162, train acc 0.9844\n",
      "Step 86300: train loss 0.0029, train acc 1.0000\n",
      "Step 86400: train loss 0.0009, train acc 1.0000\n",
      "Step 86500: train loss 0.0001, train acc 1.0000\n",
      "Step 86600: train loss 0.0044, train acc 1.0000\n",
      "Step 86700: train loss 0.1577, train acc 0.9688\n",
      "Step 86800: train loss 0.0117, train acc 1.0000\n",
      "Step 86900: train loss 0.0106, train acc 1.0000\n",
      "Step 87000: train loss 0.0143, train acc 0.9844\n",
      "Step 87000: val loss 0.0530, val acc 0.9844\n",
      "Step 87100: train loss 0.0245, train acc 0.9844\n",
      "Step 87200: train loss 0.0003, train acc 1.0000\n",
      "Step 87300: train loss 0.0837, train acc 0.9531\n",
      "Step 87400: train loss 0.0006, train acc 1.0000\n",
      "Step 87500: train loss 0.0001, train acc 1.0000\n",
      "Step 87600: train loss 0.0003, train acc 1.0000\n",
      "Step 87700: train loss 0.0008, train acc 1.0000\n",
      "Step 87800: train loss 0.0375, train acc 0.9844\n",
      "Step 87900: train loss 0.0009, train acc 1.0000\n",
      "Step 88000: train loss 0.0002, train acc 1.0000\n",
      "Step 88000: val loss 0.0163, val acc 0.9939\n",
      "Step 88100: train loss 0.0214, train acc 0.9844\n",
      "Step 88200: train loss 0.0052, train acc 1.0000\n",
      "Step 88300: train loss 0.0635, train acc 0.9844\n",
      "Step 88400: train loss 0.0024, train acc 1.0000\n",
      "Step 88500: train loss 0.0069, train acc 1.0000\n",
      "Step 88600: train loss 0.0140, train acc 0.9844\n",
      "Step 88700: train loss 0.0051, train acc 1.0000\n",
      "Step 88800: train loss 0.0111, train acc 1.0000\n",
      "Step 88900: train loss 0.0217, train acc 0.9844\n",
      "Step 89000: train loss 0.0121, train acc 0.9844\n",
      "Step 89000: val loss 0.0237, val acc 0.9925\n",
      "Step 89100: train loss 0.3466, train acc 0.9844\n",
      "Step 89200: train loss 0.0215, train acc 0.9844\n",
      "Step 89300: train loss 0.1409, train acc 0.9655\n",
      "Step 89400: train loss 0.0065, train acc 1.0000\n",
      "Step 89500: train loss 0.0002, train acc 1.0000\n",
      "Step 89600: train loss 0.0014, train acc 1.0000\n",
      "Step 89700: train loss 0.0006, train acc 1.0000\n",
      "Step 89800: train loss 0.0013, train acc 1.0000\n",
      "Step 89900: train loss 0.0138, train acc 0.9844\n",
      "Step 90000: train loss 0.0164, train acc 1.0000\n",
      "Step 90000: val loss 0.0207, val acc 0.9905\n",
      "Step 90100: train loss 0.0001, train acc 1.0000\n",
      "Step 90200: train loss 0.0003, train acc 1.0000\n",
      "Step 90300: train loss 0.0126, train acc 1.0000\n",
      "Step 90400: train loss 0.0002, train acc 1.0000\n",
      "Step 90500: train loss 0.0081, train acc 1.0000\n",
      "Step 90600: train loss 0.0363, train acc 0.9844\n",
      "Step 90700: train loss 0.0004, train acc 1.0000\n",
      "Step 90800: train loss 0.0009, train acc 1.0000\n",
      "Step 90900: train loss 0.0001, train acc 1.0000\n",
      "Step 91000: train loss 0.0005, train acc 1.0000\n",
      "Step 91000: val loss 0.0353, val acc 0.9905\n",
      "Step 91100: train loss 0.0003, train acc 1.0000\n",
      "Step 91200: train loss 0.0211, train acc 0.9844\n",
      "Step 91300: train loss 0.0384, train acc 0.9844\n",
      "Step 91400: train loss 0.0236, train acc 0.9844\n",
      "Step 91500: train loss 0.0001, train acc 1.0000\n",
      "Step 91600: train loss 0.0022, train acc 1.0000\n",
      "Step 91700: train loss 0.0001, train acc 1.0000\n",
      "Step 91800: train loss 0.0063, train acc 1.0000\n",
      "Step 91900: train loss 0.0023, train acc 1.0000\n",
      "Step 92000: train loss 0.0333, train acc 0.9844\n",
      "Step 92000: val loss 0.0154, val acc 0.9959\n",
      "Step 92100: train loss 0.0787, train acc 0.9844\n",
      "Step 92200: train loss 0.0897, train acc 0.9688\n",
      "Step 92300: train loss 0.0002, train acc 1.0000\n",
      "Step 92400: train loss 0.0010, train acc 1.0000\n",
      "Step 92500: train loss 0.0000, train acc 1.0000\n",
      "Step 92600: train loss 0.0877, train acc 0.9688\n",
      "Step 92700: train loss 0.0012, train acc 1.0000\n",
      "Step 92800: train loss 0.0165, train acc 1.0000\n",
      "Step 92900: train loss 0.0009, train acc 1.0000\n",
      "Step 93000: train loss 0.0005, train acc 1.0000\n",
      "Step 93000: val loss 0.0145, val acc 0.9918\n",
      "Step 93100: train loss 0.0834, train acc 0.9844\n",
      "Step 93200: train loss 0.0693, train acc 0.9688\n",
      "Step 93300: train loss 0.0001, train acc 1.0000\n",
      "Step 93400: train loss 0.0001, train acc 1.0000\n",
      "Step 93500: train loss 0.0005, train acc 1.0000\n",
      "Step 93600: train loss 0.0127, train acc 0.9844\n",
      "Step 93700: train loss 0.0001, train acc 1.0000\n",
      "Step 93800: train loss 0.0057, train acc 1.0000\n",
      "Step 93900: train loss 0.1065, train acc 0.9688\n",
      "Step 94000: train loss 0.0001, train acc 1.0000\n",
      "Step 94000: val loss 0.0449, val acc 0.9905\n",
      "Step 94100: train loss 0.0209, train acc 0.9844\n",
      "Step 94200: train loss 0.0032, train acc 1.0000\n",
      "Step 94300: train loss 0.0344, train acc 0.9844\n",
      "Step 94400: train loss 0.2204, train acc 0.9844\n",
      "Step 94500: train loss 0.0003, train acc 1.0000\n",
      "Step 94600: train loss 0.0053, train acc 1.0000\n",
      "Step 94700: train loss 0.0009, train acc 1.0000\n",
      "Step 94800: train loss 0.0056, train acc 1.0000\n",
      "Step 94900: train loss 0.0117, train acc 1.0000\n",
      "Step 95000: train loss 0.0023, train acc 1.0000\n",
      "Step 95000: val loss 0.0251, val acc 0.9952\n",
      "Step 95100: train loss 0.0009, train acc 1.0000\n",
      "Step 95200: train loss 0.0025, train acc 1.0000\n",
      "Step 95300: train loss 0.0009, train acc 1.0000\n",
      "Step 95400: train loss 0.0052, train acc 1.0000\n",
      "Step 95500: train loss 0.0003, train acc 1.0000\n",
      "Step 95600: train loss 0.0003, train acc 1.0000\n",
      "Step 95700: train loss 0.0004, train acc 1.0000\n",
      "Step 95800: train loss 0.0029, train acc 1.0000\n",
      "Step 95900: train loss 0.0881, train acc 0.9844\n",
      "Step 96000: train loss 0.0796, train acc 0.9844\n",
      "Step 96000: val loss 0.0140, val acc 0.9952\n",
      "Step 96100: train loss 0.0060, train acc 1.0000\n",
      "Step 96200: train loss 0.0003, train acc 1.0000\n",
      "Step 96300: train loss 0.0013, train acc 1.0000\n",
      "Step 96400: train loss 0.0009, train acc 1.0000\n",
      "Step 96500: train loss 0.0058, train acc 1.0000\n",
      "Step 96600: train loss 0.1050, train acc 0.9844\n",
      "Step 96700: train loss 0.0031, train acc 1.0000\n",
      "Step 96800: train loss 0.0004, train acc 1.0000\n",
      "Step 96900: train loss 0.0047, train acc 1.0000\n",
      "Step 97000: train loss 0.0521, train acc 0.9531\n",
      "Step 97000: val loss 0.0043, val acc 0.9973\n",
      "Step 97100: train loss 0.0496, train acc 0.9844\n",
      "Step 97200: train loss 0.0022, train acc 1.0000\n",
      "Step 97300: train loss 0.0353, train acc 0.9844\n",
      "Step 97400: train loss 0.0002, train acc 1.0000\n",
      "Step 97500: train loss 0.0003, train acc 1.0000\n",
      "Step 97600: train loss 0.0076, train acc 1.0000\n",
      "Step 97700: train loss 0.0119, train acc 1.0000\n",
      "Step 97800: train loss 0.0055, train acc 1.0000\n",
      "Step 97900: train loss 0.0003, train acc 1.0000\n",
      "Step 98000: train loss 0.0333, train acc 0.9844\n",
      "Step 98000: val loss 0.0065, val acc 0.9966\n",
      "Step 98100: train loss 0.0001, train acc 1.0000\n",
      "Step 98200: train loss 0.0852, train acc 0.9688\n",
      "Step 98300: train loss 0.0023, train acc 1.0000\n",
      "Step 98400: train loss 0.0117, train acc 0.9844\n",
      "Step 98500: train loss 0.0052, train acc 1.0000\n",
      "Step 98600: train loss 0.0048, train acc 1.0000\n",
      "Step 98700: train loss 0.0001, train acc 1.0000\n",
      "Step 98800: train loss 0.0103, train acc 1.0000\n",
      "Step 98900: train loss 0.0211, train acc 0.9844\n",
      "Step 99000: train loss 0.0006, train acc 1.0000\n",
      "Step 99000: val loss 0.0152, val acc 0.9952\n",
      "Step 99100: train loss 0.0009, train acc 1.0000\n",
      "Step 99200: train loss 0.0111, train acc 1.0000\n",
      "Step 99300: train loss 0.0034, train acc 1.0000\n",
      "Step 99400: train loss 0.0031, train acc 1.0000\n",
      "Step 99500: train loss 0.0032, train acc 1.0000\n",
      "Step 99600: train loss 0.0013, train acc 1.0000\n",
      "Step 99700: train loss 0.0010, train acc 1.0000\n",
      "Step 99800: train loss 0.0003, train acc 1.0000\n",
      "Step 99900: train loss 0.0072, train acc 1.0000\n",
      "Training completed!\n",
      "Saved final model to out/add_mod97_layer1_seed42/final_model.pt\n",
      "Saved training curves to out/add_mod97_layer1_seed42/training_curves.png\n",
      "Running experiment with seed 123\n",
      "Loading data from data/algorithmic/add_mod97\n",
      "Vocabulary size: 102\n",
      "Initializing 1-layer model\n",
      "number of parameters: 0.21M\n",
      "num decayed parameter tensors: 6, with 213,760 parameters\n",
      "num non-decayed parameter tensors: 3, with 384 parameters\n",
      "using fused AdamW: True\n",
      "/home/xsling/CSE/599s/train.py:242: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "Starting training for 100000 steps\n",
      "Step 0: train loss 4.6853, train acc 0.0000\n",
      "Step 0: val loss 4.6627, val acc 0.0143\n",
      "Step 100: train loss 4.6001, train acc 0.0000\n",
      "Step 200: train loss 4.5674, train acc 0.0000\n",
      "Step 300: train loss 4.6064, train acc 0.0156\n",
      "Step 400: train loss 4.5771, train acc 0.0156\n",
      "Step 500: train loss 4.6035, train acc 0.0156\n",
      "Step 600: train loss 4.6011, train acc 0.0000\n",
      "Step 700: train loss 4.5732, train acc 0.0156\n",
      "Step 800: train loss 4.5786, train acc 0.0000\n",
      "Step 900: train loss 4.5786, train acc 0.0156\n",
      "Step 1000: train loss 4.5962, train acc 0.0000\n",
      "Step 1000: val loss 4.5857, val acc 0.0088\n",
      "Step 1100: train loss 4.5767, train acc 0.0000\n",
      "Step 1200: train loss 4.6011, train acc 0.0000\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xsling/CSE/599s/train.py\", line 453, in <module>\n",
      "    train(args)\n",
      "  File \"/home/xsling/CSE/599s/train.py\", line 276, in train\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/home/xsling/.miniforge3/envs/599s/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/xsling/.miniforge3/envs/599s/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/xsling/.miniforge3/envs/599s/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/add_mod97 --out_dir out/add_mod97_layer1_seed${seed} --seed $seed --n_layer 1 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 1e-3 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2641639,
     "status": "ok",
     "timestamp": 1749583771945,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "5oHncWN2U6HO",
    "outputId": "57d475b3-c3d8-448b-9fe6-f15c42a402cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with seed 42\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xsling/CSE/599s/train.py\", line 452, in <module>\n",
      "    args = parse_args()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/xsling/CSE/599s/train.py\", line 445, in parse_args\n",
      "    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu', help='Device to use')\n",
      "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xsling/.miniforge3/envs/599s/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 174, in is_available\n",
      "    return torch._C._cuda_getDeviceCount() > 0\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/add_mod97 --out_dir out/add_mod97_layer2_seed${seed} --seed $seed --n_layer 2 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 1e-3 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1964161,
     "status": "ok",
     "timestamp": 1749585736108,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "4VoA2lNGU8iG",
    "outputId": "de3888a2-107a-4e31-9730-62443e65f222"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(%px): pending>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/add_mod113 --out_dir out/add_mod113_layer1_seed${seed} --seed $seed --n_layer 1 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 1e-3 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2623497,
     "status": "ok",
     "timestamp": 1749588359608,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "swpbsg0XVBp1",
    "outputId": "e54d6c5e-0a8b-4cf6-9516-e7e48aa67e37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(%px): pending>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/add_mod113 --out_dir out/add_mod113_layer2_seed${seed} --seed $seed --n_layer 2 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 1e-3 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1952832,
     "status": "ok",
     "timestamp": 1749590312442,
     "user": {
      "displayName": "玉十安",
      "userId": "16436640343490568456"
     },
     "user_tz": 420
    },
    "id": "rf1ctkeeVeM7",
    "outputId": "26fcd6aa-0cf0-4022-d38a-9f08dc209a7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(%px): pending>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/subtract_mod97 --out_dir out/subtract_mod97_layer1_seed${seed} --seed $seed --n_layer 1 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 1e-3 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "ul6rBSDiVeRZ",
    "outputId": "afce4cd0-c180-448a-c467-c0403d72a6cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(%px): pending>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/subtract_mod97 --out_dir out/subtract_mod97_layer2_seed${seed} --seed $seed --n_layer 2 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 1e-3 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1544190,
     "status": "ok",
     "timestamp": 1749598480351,
     "user": {
      "displayName": "Hank Fang",
      "userId": "09766178429983897841"
     },
     "user_tz": 420
    },
    "id": "I7LbO9mfVeV-",
    "outputId": "52eb00f1-30e4-4cd6-8534-4efc9baccc91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(%px): pending>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/subtract_mod113 --out_dir out/subtract_mod113_layer1_seed${seed} --seed $seed --n_layer 1 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 1e-3 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3Vw1GR1Veab",
    "outputId": "6cf05525-8153-4fab-b227-aac9139ce720"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(%px): pending>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/subtract_mod113 --out_dir out/subtract_mod113_layer2_seed${seed} --seed $seed --n_layer 2 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 1e-3 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlR50V5PHXLQ",
    "outputId": "0fe65fad-b3cb-47a7-f030-edeebf1668f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(%px): pending>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/divide_mod97 --out_dir out/divide_mod97_layer2_seed${seed}_batch64 --seed $seed --n_layer 2 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 1e-3 --weight_decay 1.0 --beta1 0.9 --beta2 0.98 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3MKR89WHXQ_",
    "outputId": "843f9d1d-893e-43d2-fca6-a6829540132a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(%px): pending>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!for seed in 42 123 456; do echo \"Running experiment with seed $seed\"; python train.py --data_dir data/algorithmic/divide_mod97 --out_dir out/divide_mod97_layer2_seed${seed}_batch16 --seed $seed --n_layer 2 --n_embd 128 --n_head 4 --batch_size 16 --max_steps 100000 --learning_rate 1e-3 --weight_decay 1.0 --beta1 0.9 --beta2 0.98 --log_interval 100 --eval_interval 1000; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TiE3GX3SEbuq"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "No such engine: 10",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m--targets 10 --noblock\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m!python train.py --data_dir data/algorithmic/divide_mod97 --out_dir out/divide_mod97_layer2_large_batch1 --seed 42 --n_layer 2 --n_embd 128 --n_head 4 --batch_size 256 --max_steps 100000 --learning_rate 1e-3 --weight_decay 1.0 --beta1 0.9 --beta2 0.98 --log_interval 100 --eval_interval 1000\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.miniforge3/envs/599s/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2549\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2548\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2552\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2553\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.miniforge3/envs/599s/lib/python3.12/site-packages/ipyparallel/client/magics.py:476\u001b[39m, in \u001b[36mParallelMagics.cell_px\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    474\u001b[39m block = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m args.local \u001b[38;5;28;01melse\u001b[39;00m args.block\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     ar = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_after_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43msignal_on_interrupt\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignal_on_interrupt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    486\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m args.targets:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.miniforge3/envs/599s/lib/python3.12/site-packages/ipyparallel/client/magics.py:373\u001b[39m, in \u001b[36mParallelMagics.parallel_execute\u001b[39m\u001b[34m(self, cell, block, groupby, save_name, stream_output, progress_after, signal_on_interrupt)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m    371\u001b[39m     \u001b[38;5;28mprint\u001b[39m(base + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m execution on engine(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_targets\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m result._fname = \u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mpx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28mself\u001b[39m.last_result = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.miniforge3/envs/599s/lib/python3.12/site-packages/decorator.py:235\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    234\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.miniforge3/envs/599s/lib/python3.12/site-packages/ipyparallel/client/view.py:55\u001b[39m, in \u001b[36msync_results\u001b[39m\u001b[34m(f, self, *args, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m._in_sync_results = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     ret = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m._in_sync_results = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.miniforge3/envs/599s/lib/python3.12/site-packages/decorator.py:235\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    234\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.miniforge3/envs/599s/lib/python3.12/site-packages/ipyparallel/client/view.py:39\u001b[39m, in \u001b[36msave_ids\u001b[39m\u001b[34m(f, self, *args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m n_previous = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.client.history)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     ret = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     41\u001b[39m     nmsgs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.client.history) - n_previous\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.miniforge3/envs/599s/lib/python3.12/site-packages/ipyparallel/client/view.py:658\u001b[39m, in \u001b[36mDirectView.execute\u001b[39m\u001b[34m(self, code, silent, targets, block)\u001b[39m\n\u001b[32m    655\u001b[39m block = \u001b[38;5;28mself\u001b[39m.block \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m block\n\u001b[32m    656\u001b[39m targets = \u001b[38;5;28mself\u001b[39m.targets \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m targets\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m _idents, _targets = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_build_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m futures = []\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ident \u001b[38;5;129;01min\u001b[39;00m _idents:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.miniforge3/envs/599s/lib/python3.12/site-packages/ipyparallel/client/client.py:714\u001b[39m, in \u001b[36mClient._build_targets\u001b[39m\u001b[34m(self, targets)\u001b[39m\n\u001b[32m    712\u001b[39m         targets = \u001b[38;5;28mself\u001b[39m.ids[targets]\n\u001b[32m    713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ids:\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo such engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    715\u001b[39m     targets = [targets]\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(targets, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[31mIndexError\u001b[39m: No such engine: 10"
     ]
    }
   ],
   "source": [
    "!python train.py --data_dir data/algorithmic/divide_mod97 --out_dir out/divide_mod97_layer2_seed42_batch256 --seed 42 --n_layer 2 --n_embd 128 --n_head 4 --batch_size 256 --max_steps 100000 --learning_rate 1e-3 --weight_decay 1.0 --beta1 0.9 --beta2 0.98 --log_interval 100 --eval_interval 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bzl3pINElvq"
   },
   "outputs": [],
   "source": [
    "!python train.py --data_dir data/algorithmic/divide_mod97 --out_dir out/divide_mod97_layer2_seed42_batch512 --seed 42 --n_layer 2 --n_embd 128 --n_head 4 --batch_size 512 --max_steps 100000 --learning_rate 1e-3 --weight_decay 1.0 --beta1 0.9 --beta2 0.98 --log_interval 100 --eval_interval 1000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Refactoring Summary\n",
    "\n",
    "## 1. Experiment Naming Convention\n",
    "- `_small_batch` → `_batch16` (batch size = 16)\n",
    "- `large_batch1` → `_batch256` (batch size = 256)  \n",
    "- `large_batch2` → `_batch512` (batch size = 512)\n",
    "- Default experiments use `_batch64` (batch size = 64)\n",
    "\n",
    "## 2. Plotting Improvements\n",
    "- Plots now show experiment name with operation, moduli, and batch size in the title\n",
    "- X-axis uses logarithmic scale with exponential notation (10¹, 10², ..., 10⁶)\n",
    "- X-axis label changed to \"Optimization Steps\"\n",
    "- Added grid for better readability\n",
    "- Fixed x-axis range to properly show up to 10⁶ steps\n",
    "\n",
    "## 3. Tokenization Changes\n",
    "- Changed from character-level to number-level tokenization\n",
    "- Numbers are now single tokens (e.g., '123' is one token, not three)\n",
    "- **Removed all special tokens** (`<bos>`, `<eos>`, `<pad>`) - no longer needed\n",
    "- Example: `12+24=36` → `['12', '+', '24', '=', '36']`\n",
    "- For arithmetic tasks, set `max_new_tokens=1` during inference since we predict one number token at a time\n",
    "\n",
    "## Note on Compatibility\n",
    "The new tokenization scheme is NOT compatible with models trained using the old tokenizer. You'll need to retrain all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new number-level tokenization\n",
    "from train import NumberTokenizer\n",
    "\n",
    "# Example equations (no special tokens needed)\n",
    "test_equations = [\n",
    "    \"12+24=36\",\n",
    "    \"100-57=43\",\n",
    "    \"8/2=4\"\n",
    "]\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = NumberTokenizer(test_equations)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Tokens: {tokenizer.tokens}\")\n",
    "print(\"\\nTokenization examples:\")\n",
    "\n",
    "for eq in test_equations:\n",
    "    encoded = tokenizer.encode(eq)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"\\nOriginal: {eq}\")\n",
    "    print(f\"Encoded: {encoded}\")\n",
    "    print(f\"Tokens: {[tokenizer.idx_to_token[idx] for idx in encoded]}\")\n",
    "    print(f\"Decoded: {decoded}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "599s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
