Experiment: subtract_mod113_layer1_seed456
Started at: 2025-06-11 22:22:48
Command: python train.py --data_dir data/algorithmic/subtract_mod113 --out_dir out/subtract_mod113_layer1_seed456 --seed 456 --n_layer 1 --n_embd 128 --n_head 4 --batch_size 64 --max_steps 100000 --learning_rate 0.001 --log_interval 1000 --eval_points_per_decade 16
============================================================

Will evaluate at 56 logarithmically spaced steps
Loading data from data/algorithmic/subtract_mod113
Using NumberTokenizer for arithmetic data
Vocabulary size: 118
Initializing 1-layer model
number of parameters: 0.21M
num decayed parameter tensors: 6, with 215,808 parameters
num non-decayed parameter tensors: 3, with 384 parameters
using fused AdamW: True
Starting training for 100000 steps
Step 0: train loss 4.7954, train acc 0.0000
Step 0: val loss 4.8194, val acc 0.0094
Training completed!
Saved final model to out/subtract_mod113_layer1_seed456/final_model.pt
Saved training curves to out/subtract_mod113_layer1_seed456/training_curves.png

============================================================
Completed at: 2025-06-11 22:30:00
Return code: 0
